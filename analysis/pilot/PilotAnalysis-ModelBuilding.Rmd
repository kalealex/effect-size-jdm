---
title: "PilotAnalysis-ModelBuilding"
author: "Alex Kale"
date: "2/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(RColorBrewer)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(modelr)
library(tidybayes)
library(brms)
```

#Exploratory Visualization

This section is intended to give an overview of the response distributions from our pilot.

##Data

###Load Worker Responses from Pilot

```{r}
# read in data 
responses_df <- read_csv("pilot-anonymous.csv")
# rename to convert away from camel case
responses_df <- responses_df %>%
  rename(
    ground_truth=groundTruth,
    sd_diff=sdDiff,
    worker_id=workerId,
    start_time=startTime,
    resp_time=respTime,
    trial_dur=trialDur
  ) %>%
  mutate(
    trial_dur = ifelse(trial_dur < 0, 0, trial_dur), # avoid negative trial durations from faulty reconstruction (only one case)
    cles = ifelse(cles == 0, 0.25, cles),            # avoid responses equal to zero
    cles = ifelse(cles == 100, 99.75, cles),         # avoid responses equal to one-hundred
    bet = ifelse(bet == 1000, 999.75, bet)           # avoid responses equal to one-thousand
  ) 

head(responses_df)
```

###Load Stimuli-Generating Data

```{r}
# data used to create stimuli
load("./conds_df.Rda")
```


##Response Distributions

###CLES Judgments

As we would expect based on the ubiquitous linear log odds representation of probability, CLES judgments tend to be biased toward 50% relative to the ground truth. This is not so much the case for HOPs, howeverm responses are highly variable.

```{r}
for (cond in unique(responses_df$condition)) {
  plt <- responses_df %>% filter(condition == cond) %>%
    ggplot(aes(x=cles)) +
    geom_vline(aes(xintercept=ground_truth*100, linetype="Ground Truth"), color="red") +
    scale_linetype_manual(name="Line", values = c(2,1), guide=guide_legend(override.aes=list(color=c("red")))) +
    geom_histogram(aes(y=..density..), binwidth=5) +
    # geom_density(fill = "#ff4d4d", alpha = 0.2) +
    theme_bw() +
    labs(
      caption=cond,
      x = "CLES Responses",
      y = "Frequency"
    ) +
    facet_grid(sd_diff ~ ground_truth)
  print(plt)
}
```

###Bet Amounts

Bet amounts seem more sensitive to probability information when sd_diff is high, making uncertainty more visually salient. However, bet amounts are highly variable across the board.

```{r}
for (cond in unique(responses_df$condition)) {
  plt <- responses_df %>% filter(condition == cond) %>%
    ggplot(aes(x=bet)) +
    geom_vline(aes(xintercept=ground_truth*1000, linetype="Optimal Bet"), color="red") +
    scale_linetype_manual(name="Line", values = c(2,1), guide=guide_legend(override.aes=list(color=c("red")))) +
    geom_histogram(aes(y=..density..), binwidth=50) +
    # geom_density(fill = "#ff4d4d", alpha = 0.2) +
    theme_bw() +
    labs(
      caption=cond,
      x = "Bet Amount",
      y = "Frequency"
    ) +
    facet_grid(sd_diff ~ ground_truth)
  print(plt)
}
```

###CLES Judgments vs Bet Amounts

Under an ideal betting strategy, bet amounts should be 10 times the CLES value perceived by the participant. We can see that for intervals_w_means and means_only---visualizations where the mean is emphasized---bet amounts are too high for CLES responses above 50% and too low for CLES responses below 50%. In other words, bet amount is too sensitive to perceived probability of winning. Contrast this with HOPs, where we see the same pattern to a lesser extent and bet amount looks more like a noisy linear function of the CLES response.

```{r}
for (cond in unique(responses_df$condition)) {
  plt <- responses_df %>% filter(condition == cond) %>%
    ggplot(aes(x=cles, y=bet)) +
    geom_abline(intercept=0, slope=10, color="red", linetype="dashed") +
    # scale_linetype_manual(name="Line", values = c(2,1), guide=guide_legend(override.aes=list(color=c("red")))) +
    geom_point(alpha=0.3) +
    theme_bw() +
    labs(
        caption=cond,
        x = "CLES Judgment",
        y = "Bet Amount"
    ) +
    facet_grid(sd_diff ~ ground_truth)
  print(plt)
}
```

###Relationships with Trial Duration

We want to know when, if at all, spending more time on a response results in improved performance.

####Trial Duration vs CLES Judgments

Trial duration seems mostly unrelated to CLES judgments except for in the case of HOPs, where responses seem to cluster closer to the ground truth on longer trial durations.

```{r}
for (cond in unique(responses_df$condition)) {
  plt <- responses_df %>% filter(condition == cond) %>%
    ggplot(aes(x=trial_dur, y=cles)) +
    geom_hline(aes(yintercept=ground_truth*100, linetype="Ground Truth"), color="red") +
    scale_linetype_manual(name="Line", values = c(2,1), guide=guide_legend(override.aes=list(color=c("red")))) +
    geom_point(alpha=0.3) +
    theme_bw() +
    labs(
        caption=cond,
        x = "Trial Duration (sec)",
        y = "CLES Judgment"
    ) +
    facet_grid(sd_diff ~ ground_truth)
  print(plt)
}
```

####Trial Duration vs Bet Amounts

Similar to what we see above with CLES judgments, trial duration seems mostly unrelated to bet amounts except for in the case of HOPs, where responses seem to cluster closer to the optimal bet on longer trial durations.

```{r}
for (cond in unique(responses_df$condition)) {
  plt <- responses_df %>% filter(condition == cond) %>%
    ggplot(aes(x=trial_dur, y=bet)) +
    geom_hline(aes(yintercept=ground_truth*1000, linetype="Optimal Bet"), color="red") +
    scale_linetype_manual(name="Line", values = c(2,1), guide=guide_legend(override.aes=list(color=c("red")))) +
    geom_point(alpha=0.3) +
    theme_bw() +
    labs(
        caption=cond,
        x = "Trial Duration (sec)",
        y = "Bet Amount"
    ) +
    facet_grid(sd_diff ~ ground_truth)
  print(plt)
}
```


##Responses and Ground Truth vs Heuristic Predictions

### Create Heuristic Functions

The following functions describe the CLES responses predicted by possible heuristics for reading the visualizations in our pilot.

```{r}
# axis range for modeling
data_domain <- c(38, 62)
axis_range <- data_domain[2] - data_domain[1]

# relative mean difference heuristic
relative_mean_difference <- function(mean_diff, max_abs_mean_diff) {
  return(50 - 50 * mean_diff / max_abs_mean_diff)
}

# mean difference relative to axis range
mean_difference_vs_axis <- function(mean_diff) {
  return(50 - 50 * mean_diff / axis_range)
}

# means first, then uncertainty heuristic
means_first_then_uncertainty_intervals <- function(mean_diff, sd_team) {
  interval_length <- qnorm(0.975)*sd_team - qnorm(0.025)*sd_team 
  return(50 - 50 * mean_diff / interval_length / 2) # assuming that the two intervals are the same length, so we don't need to take their average
}

# interval overlap relative to interval length
interval_overlap <- function(mean_diff, sd_team) {
  interval_length <- qnorm(0.975)*sd_team - qnorm(0.025)*sd_team # baseline for relative judgment (assuming that the two intervals are the same length, so we don't need to take their average)
  mean_teamA <- - mean_diff / 2 # relative to center
  mean_teamB <- mean_diff / 2 # relative to center
  # calculation depends on which mean is larger
  if(mean_teamA > mean_teamB) {
    interval_overlap <- (mean_teamB + interval_length / 2) - (mean_teamA - interval_length / 2) # upper bound of lower dist minus lower bound of higher dist
    return(100 - 50 * interval_overlap / interval_length) 
  } else { # mean_teamA < mean_teamB
    interval_overlap <- (mean_teamA + interval_length / 2) - (mean_teamB - interval_length / 2) # upper bound of lower dist minus lower bound of higher dist
    return( 50 * interval_overlap / interval_length)
  }
}

# interval overlap relative to axis range
interval_overlap_vs_axis <- function(mean_diff, sd_team) {
  interval_length <- qnorm(0.975)*sd_team - qnorm(0.025)*sd_team # baseline for relative judgment (assuming that the two intervals are the same length, so we don't need to take their average)
  mean_teamA <- - mean_diff / 2 # relative to center
  mean_teamB <- mean_diff / 2 # relative to center
  # calculation depends on which mean is larger
  if(mean_teamA > mean_teamB) {
    interval_overlap <- (mean_teamB + interval_length / 2) - (mean_teamA - interval_length / 2) # upper bound of lower dist minus lower bound of higher dist
    return(100 - 50 * interval_overlap / axis_range) 
  } else { # mean_teamA < mean_teamB
    interval_overlap <- (mean_teamA + interval_length / 2) - (mean_teamB - interval_length / 2) # upper bound of lower dist minus lower bound of higher dist
    return( 50 * interval_overlap / axis_range)
  }
}

# outcome proportion heuristic
outcome_proportion <- function(draws) {
  return(100 * sum(draws < 0) / length(draws))
}

# means over sd from HOPs heuristic
means_first_then_uncertainty_hops <- function(draws) {
  # get summary statistics from differences between draws
  mean_diff <- mean(draws)
  outcome_diff_span <- max(draws) - min(draws)
  outcome_span <- sqrt((outcome_diff_span ^ 2) / 2)
  return(50 - 50 * mean_diff / outcome_span / 2)
}

# need a consistent color scale for these heuristics
heuristics <- as.factor(c("ground_truth", 
                          "rel_mean_diff_est", "mean_diff_vs_axis_est", 
                          "means_first_then_uncertainty_intervals_est", "interval_overlap_est", "interval_overlap_vs_axis_est", 
                          "outcome_proportion_est",  "outcome_proportion_10_est", "means_first_then_uncertainty_hops_est"))
# hColors <- brewer.pal(length(heuristics), "Set1")
hColors <- c("#E31A1C",                         # from ColorBrewer 12-class Paired palette
              "#B2DF8A", "#FDBF6F",
              "#6A3D9A", "#1F78B4", "#33A02C", 
              "#CAB2D6", "#B15928", "#A6CEE3"
              )
names(hColors) <- levels(heuristics)
colScale <- scale_colour_manual(values = hColors)
```

###Create Optimal Betting Functions

These functions define the optimal betting strategy. However, for any given CLES value, the optimal bet is $1000 coins * Pr(A > B)$.

```{r}
# set range of possible bets based on given budget and minimum bet
budget <- 1000
min_bet <- 1
possible_bets <- seq(from=min_bet, to=budget, by=1)

# create a tiered capital gains tax
tax_winnings <- function(winnings) {
  tiers <- append(seq(0, 2000, by = 500), Inf)
  rates <- seq(0, .5, by = .1)
  taxed_winnings <- sum(diff(c(0, pmin(winnings, tiers))) * (1-rates))
  return(taxed_winnings)
}

# set cost of not betting
loss_rate <- 0.25

# find the optimal bet based on the expected value of bet amounts given some CLES value
optimal_bet <- function(p_superiority_A) {
  # hack to catch p == 0
  if (p_superiority_A == 0) {
    p_superiority_A <- 0.001
  }
  # calculate utility over as set of possible bets at the given odds
  utility <- seq(from=-1, to=0, length.out = length(possible_bets))
  for (i in 1:length(possible_bets)) {
    utility[i] <- (1 - loss_rate)*(budget - possible_bets[i]) + p_superiority_A * tax_winnings(possible_bets[i] / p_superiority_A) # payoff proportional to risk
  }
  # determine the bet with the maximum expected utility
  return(possible_bets[which(utility==max(utility))])
}
```

###Apply Prediction Functions to Data Conditions Merge With Response Data 

This section of code combines the responses and stimuli-generating data into one visualization that we can use to analyze the prevalence of different heuristics.

```{r}
# calcate the difference in draws for the heuristic functions
draw_differences <- conds_df %>% select(condition, Team, draws) %>% 
  spread(Team, draws) %>% 
  unnest() %>% 
  mutate(
    draws_diff=B - A, 
    A=NULL, 
    B=NULL
  ) %>% 
  group_by(condition) %>% 
  summarise(draws_diff = list(draws_diff[1:50]))

# reformat data conditions df
stimuli_data_df <- conds_df %>% 
  filter(Team %in% "A") %>% # drop duplicate rows for two teams
  left_join(draw_differences, by='condition') %>%
  mutate( # drop unnecessary columns
    condition=NULL,
    Team=NULL, 
    draws=NULL,
    draw_n=NULL,
    quantiles=NULL,
    sample_n=NULL
  )

# repeat heuristics data frame for each worker 
stimuli_data_df <- stimuli_data_df[rep(seq_len(nrow(stimuli_data_df)), times=length(unique(responses_df$worker_id))),]
stimuli_data_df$worker_id <- sort(rep(unique(responses_df$worker_id), each=(length(unique(responses_df$ground_truth))) * length(unique(responses_df$sd_diff))))

# calculate the baseline of relative mean difference heuristic)
stimuli_data_df$max_abs_mean_diff <- max(abs(stimuli_data_df$mean_diff))
```

```{r}
# create dataframe containing heuristic estimates
heuristics_df <- stimuli_data_df %>% rowwise() %>% 
  mutate( # call heuristic functions
    ground_truth = odds_of_victory * 100,
    rel_mean_diff_est = relative_mean_difference(mean_diff, max_abs_mean_diff),
    mean_diff_vs_axis_est = mean_difference_vs_axis(mean_diff),
    means_first_then_uncertainty_intervals_est = means_first_then_uncertainty_intervals(mean_diff, sd),
    interval_overlap_est = interval_overlap(mean_diff, sd),
    interval_overlap_vs_axis_est = interval_overlap_vs_axis(mean_diff, sd),
    outcome_proportion_est = outcome_proportion(draws_diff),
    outcome_proportion_10_est = outcome_proportion(draws_diff[1:10]), # outcome proportion with only the first 10 draws
    means_first_then_uncertainty_hops_est = means_first_then_uncertainty_hops(draws_diff)
  ) %>% 
  gather(heuristic, est_cles, ground_truth, rel_mean_diff_est, mean_diff_vs_axis_est, means_first_then_uncertainty_intervals_est, interval_overlap_est, interval_overlap_vs_axis_est, outcome_proportion_est, outcome_proportion_10_est, means_first_then_uncertainty_hops_est) %>% # reshape
  rowwise() %>%
  mutate(est_bet = optimal_bet(est_cles / 100)) %>% # apply optimal bet function (if multiple optimal bets, take the lower to avoid error)
  rename(ground_truth = odds_of_victory) %>%
  arrange(worker_id, sd_diff, ground_truth, heuristic) # use same order for both data frames

# extend responses df to repeat for each heuristic
combined_df <- responses_df[rep(seq_len(nrow(responses_df)), each=length(unique(heuristics_df$heuristic))),]
combined_df$heuristic <- rep(rep(unique(heuristics_df$heuristic), times=(length(unique(responses_df$ground_truth))) * length(unique(responses_df$sd_diff))), times=length(unique(responses_df$worker_id)))

# merge response data with heuristics data
combined_df <- combined_df %>% 
  arrange(worker_id, sd_diff, ground_truth, heuristic) %>% # use same order for both data frames
  bind_cols(heuristics_df) # hack because merge and join not working
  # left_join(heuristics_df, by=c('worker_id','ground_truth', 'sd_diff','heuristic'))
  # merge(heuristics_df, on=c('worker_id','ground_truth', 'sd_diff','heuristic'), all=TRUE)

# check the binding
if (!all(combined_df$worker_id == combined_df$worker_id1) || !all(combined_df$heuristic == combined_df$heuristic1) || 
    !all(combined_df$sd_diff == combined_df$sd_diff1) || !all(combined_df$ground_truth == round(combined_df$ground_truth1, 3))) {
  print("Warning: something went horribly wrong!")
}
```

### Plot Responses Against Heuristics

These visualizations allow us to check responses from individual workers against the predictions of the set of heuristics which are relevant to each visualization condition.

#### CLES Judgments

Through visual inspection of the plots below, I tallied up the apparent strategy for each worker in each level of sd_diff. Since visualization condition is a between subjects manipulation, no individual worker is contributing more than two strategy codes.

In the HOPs condition, workers seem to be using an outcome_proportion heuristic for only the first 10 draws about half of the time. The other half of the time, I cannot distinguish their strategy (a.k.a., ambiguous strategy) or they are using a means_first heuristic where they estimate the mean difference from the draws to inform their reliability judgment and then compare that to the average span of draws for each of the two teams. Only one worker seemed to actually be counting the proportion of all draws shown where $A > B$. Switching strategies depending on sd_diff was uncommon. When workers seem to have switched strategies, they appeared to be using a means_first heuristic or an ambiguous strategy at low levels of uncertainty and then switching to an outcome_proportion heuristics when sd_diff was high.

In the means_only condition, workers seems to be using a mean_diff_vs_axis heuristic more than half the time. However, we also see many workers guessing CLES values near 50% regardless of the stimulus condition, a pattern which is indistinguishable from the mean_diff_vs_axis heuristic at low levels of sd_diff. Only a couple workers seem to be basing their sense of what makes an effect reliable on the relative mean difference (compared to the maximum mean difference shown), rather than the range of the x_axis.

In the intervals_w_means condition, workers seem to switch strategies the most, with the exception of a couple workers who seemed to consistently rely on the interval_overlap and relative_mean_difference heuristics. Similar the means_only condition a subset of workers seem to guess CLES values near 50% regardless of the stimulus condition, a pattern which is indistinguishable from the mean_diff_vs_axis heuristic at low levels of sd_diff. Although my coding scheme did not account for changing stratgies depending on the ground truth, it seems like this may be happening for some participants in this condition. Specifically, there are a few participants who overestimate small probabilities more than they underestimate large probabilities.

```{r}
# plot predictions w/ participant responses
for (worker in unique(combined_df$worker_id)) {
  # filter on worker
  worker_data <- combined_df %>% filter(worker_id == worker)
  # title <- cat("Heuristic Predictions vs Ground Truth w/", worker, "Estimates of CLES")

  # filter heuristics based on condition (between subjects)
  if (worker_data$condition[1] %in% "HOPs") {
    plt <- worker_data %>% filter(heuristic %in% c("ground_truth", "means_first_then_uncertainty_hops_est", "outcome_proportion_est",  "outcome_proportion_10_est")) %>%
      ggplot(aes(x=ground_truth, y=est_cles, color=heuristic)) +
      geom_line() +
      geom_point(aes(x=ground_truth, y=cles), inherit.aes=FALSE, show.legend=FALSE) +
      colScale +
      theme_bw() +
      labs(
          x = "Ground Truth Pr(A > B)",
          y = "Estimated Pr(A > B)"
      ) +
      facet_grid(sd_diff ~ condition)
    print(plt)
  } else if (worker_data$condition[1] %in% "means_only") {
    plt <- worker_data %>% filter(heuristic %in% c("ground_truth", "rel_mean_diff_est", "mean_diff_vs_axis_est")) %>%
      ggplot(aes(x=ground_truth, y=est_cles, color=heuristic)) +
      geom_line() +
      geom_point(aes(x=ground_truth, y=cles), inherit.aes=FALSE, show.legend=FALSE) +
      colScale +
      theme_bw() +
      labs(
          x = "Ground Truth Pr(A > B)",
          y = "Estimated Pr(A > B)"
      ) +
      facet_grid(sd_diff ~ condition)
    print(plt)
  } else { # intervals
    plt <- worker_data %>% filter(heuristic %in% c("ground_truth", "rel_mean_diff_est", "mean_diff_vs_axis_est", "means_first_then_uncertainty_intervals_est", "interval_overlap_est", "interval_overlap_vs_axis_est")) %>%
      ggplot(aes(x=ground_truth, y=est_cles, color=heuristic)) +
      geom_line() +
      geom_point(aes(x=ground_truth, y=cles), inherit.aes=FALSE, show.legend=FALSE) +
      colScale +
      theme_bw() +
      labs(
          caption=worker,
          x = "Ground Truth Pr(A > B)",
          y = "Estimated Pr(A > B)"
      ) +
      facet_grid(sd_diff ~ condition)
    print(plt)
  }
}

# heuristics_df %>% ggplot(aes(x=odds_of_victory, y=est_cles, color=heuristic)) +
#   geom_line() +
#   geom_point(data=responses_df, aes(x=groundTruth, y=cles, alpha=0.3), inherit.aes=FALSE, show.legend=FALSE) +
#   colScale +
#   theme_bw() +
#   labs(title = "Heuristic Predictions vs Ground Truth w/ Worker Estimates of CLES",
#       x = "Ground Truth Pr(A > B)",
#       y = "Estimated Pr(A > B)"
#   ) +
#   facet_grid(sd_diff ~ condition)
```

#### Bet Amounts

Since the betting data is noisier than the CLES judgments, I did not tally strategies for these responses. By visual inspection of the plots below, my sense is that many participants tend to bet amounts either near 0, near 500, or near 1000 coins. Workers who make consistent bets despite varying probability of victory seem to break out of their pattern most often when odds are extreme (e.g., opting to bet more when they are sure they will win or bet less when they are sure they will lose). Some workers seem to employ more of an optimal betting strategy, mostly workers in the HOPs and intervals_w_means conditions, especially when sd_diff is large. For most workers, bets seem to show a curvelinear relationship with the ground truth rather than the optimal linear relationship. This brings to mind the possibility that a linear log odds model would account for betting behavior.

```{r}
# plot predictions w/ participant responses
for (worker in unique(combined_df$worker_id)) {
  # filter on worker
  worker_data <- combined_df %>% filter(worker_id == worker)
  # title <- cat("Heuristic Predictions vs Ground Truth w/", worker, "Estimates of CLES")

  # filter heuristics based on condition (between subjects)
  if (worker_data$condition[1] %in% "HOPs") {
    plt <- worker_data %>% filter(heuristic %in% c("ground_truth", "means_first_then_uncertainty_hops_est", "outcome_proportion_est",  "outcome_proportion_10_est")) %>%
      ggplot(aes(x=ground_truth, y=est_bet, color=heuristic)) +
      geom_line() +
      geom_point(aes(x=ground_truth, y=bet), inherit.aes=FALSE, show.legend=FALSE) +
      colScale +
      theme_bw() +
      labs(
          x = "Ground Truth Pr(A > B)",
          y = "Bet Amount"
      ) +
      facet_grid(sd_diff ~ condition)
    print(plt)
  } else if (worker_data$condition[1] %in% "means_only") {
    plt <- worker_data %>% filter(heuristic %in% c("ground_truth", "rel_mean_diff_est", "mean_diff_vs_axis_est")) %>%
      ggplot(aes(x=ground_truth, y=est_bet, color=heuristic)) +
      geom_line() +
      geom_point(aes(x=ground_truth, y=bet), inherit.aes=FALSE, show.legend=FALSE) +
      colScale +
      theme_bw() +
      labs(
          x = "Ground Truth Pr(A > B)",
          y = "Bet Amount"
      ) +
      facet_grid(sd_diff ~ condition)
    print(plt)
  } else { # intervals
    plt <- worker_data %>% filter(heuristic %in% c("ground_truth", "rel_mean_diff_est", "mean_diff_vs_axis_est", "means_first_then_uncertainty_intervals_est", "interval_overlap_est", "interval_overlap_vs_axis_est")) %>%
      ggplot(aes(x=ground_truth, y=est_bet, color=heuristic)) +
      geom_line() +
      geom_point(aes(x=ground_truth, y=bet), inherit.aes=FALSE, show.legend=FALSE) +
      colScale +
      theme_bw() +
      labs(
          x = "Ground Truth Pr(A > B)",
          y = "Bet Amount"
      ) +
      facet_grid(sd_diff ~ condition)
    print(plt)
  }
}

# heuristics_df %>% ggplot(aes(x=odds_of_victory, y=est_bet, color=heuristic)) +
#   geom_line() +
#   geom_point(data=responses_df, aes(x=groundTruth, y=bet, alpha=0.3), inherit.aes=FALSE, show.legend=FALSE) +
#   colScale +
#   theme_bw() +
#   labs(title = "Heuristic Predictions vs Ground Truth w/ Worker Bets",
#       x = "Ground Truth Pr(A > B)",
#       y = "Estimated Bet Amount"
#   ) +
#   facet_grid(sd_diff ~ condition)
```


##Error Analysis

In this section, we look for patterns of interest in response errors.

```{r}
# calculate error and absolute error, add to df
combined_df <- combined_df %>% 
  mutate(
    err_cles = est_cles - cles,
    abs_err_cles = abs(err_cles),
    err_bet = est_bet - bet,
    abs_err_bet = abs(err_bet)
  )
```

###Mean Error Per Visualization Condition

Collapsing across data conditions is a little reductive, but it is probably important to look at the overall pattern of absolute errors across visualization conditions.

On average, errors in CLES judgments are smallest in the HOPs condition.

```{r}
# avg absolute error per condition
combined_df %>% 
  filter(heuristic %in% "ground_truth") %>%
  group_by(condition) %>%
  summarise(avg_abs_err_cles = mean(abs_err_cles)) %>%
  ggplot(aes(x=condition, y=avg_abs_err_cles, fill=condition)) +
    geom_bar(stat="identity") +
    theme_bw() +
    labs(title = "Average Absolute Error Relative to Ground Truth",
        x = "Visualization Condition",
        y = "Average Absolute Error"
    )
```

On average, errors in bet amounts seem relatively similar across visualization conditions.

```{r}
# avg absolute error per condition
combined_df %>% 
  filter(heuristic %in% "ground_truth") %>%
  group_by(condition) %>%
  summarise(avg_abs_err_bet = mean(abs_err_bet)) %>%
  ggplot(aes(x=condition, y=avg_abs_err_bet, fill=condition)) +
    geom_bar(stat="identity") +
    theme_bw() +
    labs(title = "Average Absolute Error Relative to Optimal Bet",
        x = "Visualization Condition",
        y = "Average Absolute Error"
    )
```

###Mean Error vs Ground Truth

Looking at the average signed errors in CLES estimates by condition, we can see that HOPs lead to less biased CLES judgments for extreme probabilities. 

```{r}
# error by ground truth, per condition
combined_df %>%
  filter(heuristic %in% "ground_truth") %>%
  group_by(sd_diff, ground_truth, condition) %>%
  summarise(avg_err_cles = mean(err_cles)) %>%
  ggplot(aes(x=ground_truth, y=avg_err_cles, color=condition)) +
    geom_line() +
    theme_bw() +
    labs(title = "Average Error Relative to Ground Truth",
        x = "Ground Truth Pr(A > B)",
        y = "Average Error"
    ) +
    facet_grid(sd_diff ~ .)
```

The average signed errors in bet amounts by condition show a more complex pattern. Note that there is a general bias toward betting too much except when the probability of winning is below 25%. This bias seems absent in the intervals_w_means condition when sd_diff is low, and this bias seems most consistent in the HOPs condition. However, it is hard to know whether these patterns are robust.

```{r}
# error by ground truth, per condition
combined_df %>%
  filter(heuristic %in% "ground_truth") %>%
  group_by(sd_diff, ground_truth, condition) %>%
  summarise(avg_err_bet = mean(err_bet)) %>%
  ggplot(aes(x=ground_truth, y=avg_err_bet, color=condition)) +
    geom_line() +
    theme_bw() +
    labs(title = "Average Error Relative to Optimal Bet",
        x = "Ground Truth Pr(A > B)",
        y = "Average Error"
    ) +
    facet_grid(sd_diff ~ .)
```

####Check Bias Depending on Winner of Game

We want to know whether the probable winner of the game (i.e., whether ground truth CLES is greater than or less than 50%) has an impact on responses. This would show up as an asymmetry between errors depending on the winner of the game.

The chart of average signed errors for CLES judgments below shows such an asymmetry for the HOPs condition especially. In particular, HOPs seem less biased than other conditions, particularly when the ground truth CLES is less than 50%. However, it is hard to tell whether this relationship is robust.

```{r}
# reflect error where Pr(A > B) < 0.5 onto range between 0.5 and 1
combined_df %>%
  filter(heuristic %in% "ground_truth") %>%
  mutate(
    ground_truth_50_100 = ifelse(ground_truth < 0.5, 1 - ground_truth, ground_truth),
    winner = ifelse(outcome == "True", "A", "B")
  ) %>%
  group_by(sd_diff, ground_truth_50_100, condition, winner) %>%
  summarise(avg_err_cles = mean(err_cles)) %>%
  ggplot(aes(x=ground_truth_50_100, y=avg_err_cles, color=condition)) +
    geom_line(aes(linetype=winner)) +
    theme_bw() +
    labs(title = "Average Error in CLES Judgments Relative to Probability of Superiority for Winner",
        x = "Ground Truth Pr(Win)",
        y = "Average Error"
    ) +
    facet_grid(sd_diff ~ .)
```

As we've seen with other metrics, bet amounts show a more complex pattern. Again, we can see that HOPs seem to promote betting too much when the ground truth CLES is close to 50%. It is hard to tell whether the minor asymmetries in this plot are meaningful.

```{r}
# reflect error where Pr(A > B) < 0.5 onto range between 0.5 and 1
combined_df %>%
  filter(heuristic %in% "ground_truth") %>%
  mutate(
    ground_truth_50_100 = ifelse(ground_truth < 0.5, 1 - ground_truth, ground_truth),
    winner = ifelse(outcome == "True", "A", "B")
  ) %>%
  group_by(sd_diff, ground_truth_50_100, condition, winner) %>%
  summarise(avg_err_bet = mean(err_bet)) %>%
  ggplot(aes(x=ground_truth_50_100, y=avg_err_bet, color=condition)) +
    geom_line(aes(linetype=winner)) +
    theme_bw() +
    labs(title = "Average Error in Bet Amounts Relative to Probability of Superiority for Winner",
        x = "Ground Truth Pr(Win)",
        y = "Average Error"
    ) +
    facet_grid(sd_diff ~ .)
```



#Building a Model of the Probability of Each Heuristic

We start out with a simple non-hierarchical model for the probability of one heuristic vs ground truth in one visualization condition, HOPs. Then we model the probability of more than one alternative heuristic. Then we'll add hierarchy to that model. Finally, we'll add in the other visualization conditions.


##Prepare the Data Frame for Modeling in Stan

First, we need the data in a format where it is prepared for modeling in Stan. We will calculate the heuristic predictions in Stan, so we just need to get the stimuli-generating data and the worker response data in a single dataframe with one row per worker * trial.

```{r}
# create data frame for model by merging stimuli-generating data with responses
model_df <- stimuli_data_df %>%
  mutate( # create rounded version of ground_truth to merge on, leaving unrounded value stored in odds_of_victory
    ground_truth=round(odds_of_victory, 3)
  ) %>%
  full_join(responses_df, by=c("worker_id", "sd_diff", "ground_truth")) %>%
  rename( # rename ground_truth columns, so it is clear which is rounded and which should be used in the model
    ground_truth_rounded=ground_truth,
    ground_truth=odds_of_victory
  )
```


##Non-Hierarchical Model of the Probability of the Outcome Proportion Heuristic vs Ground Truth in the HOPs Condition

We start with the simplest possible version of the model we'd like to build. Basically, we are modeling the probability that a worker in the HOPs condition will use the outcome proportion heuristic vs the ground truth. This model does not differentiate between workers (no hierarchy, i.e., random effect of worker), focuses on one visualization condition, and ignores all but two possible heuristics. It is a first step toward a much more sophisticated model.

###Prepare Data List for Modeling in Stan

Include only the HOPs condition.

```{r}
# filter condition
model_df_hops <- model_df %>% filter(condition=="HOPs")

# create data list
data_hops_one <- list(
  n=length(model_df_hops$trial),                  # total observations (i.e., trials)
  cles=model_df_hops$cles,                        # the worker's response on each trial
  ground_truth=model_df_hops$ground_truth,        # the ground truth cles value on each trial
  n_draws=50,                                     # number of draws displayed in the HOPs per trial
  # draws=model_df_hops$draws_diff                  # multidimensional array of the difference between draws B - A shown in HOPs trials * draws
  draws=do.call("rbind",model_df_hops$draws_diff) # trials * draws matrix of the difference between draws B - A shown in HOPs
)
```

###Compile and Run the Model

The model is in the file "stan/hops-only_one-heuristic.stan". We'll compile the stan code, convert to a model object, and then fit the model.

```{r echo=FALSE}
# compile stan code and create model object
stan_model = stan_model("stan/hops-only_one-heuristic.stan")
```

```{r}
# fit model
m.hops.one_heuristic <- sampling(stan_model, data=data_hops_one, control=list(adapt_delta=0.99), chain=2, cores=2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
traceplot(m.hops.one_heuristic, pars = c('p_heuristic', 'sigma'))
```

- Pairs plot. These look a little skewed, but that might be just because sigma is bounded.

```{r}
# pairs plot
pairs(m.hops.one_heuristic, pars = c('p_heuristic', 'sigma'))
```

- Summary

```{r}
# model summary
print(m.hops.one_heuristic)
```

The model has learned that users seem fairly evenly split between the outcome proportion heuristic and the ground truth when we only allow for those two strategies. The high variability of this estimate suggests that there is a lot of information which is not accounted for by this model (which we would expect since this binary choice of strategy probably does not represent the true data-generating process).

<!-- - Posterior predictive distribution -->

<!-- ```{r} -->
<!-- # posterior predictive check -->
<!-- m.hops.one_heuristic %>% -->
<!--   spread_draws(p_heuristic, sigma, heuristic_est) %>% -->
<!--   mutate( -->
<!--     use_heuristic = rbernoulli(n(), p_heuristic), -->
<!--     ly_rep = plogis(rnorm(heuristic_est[use_heuristic,], sigma)) -->
<!--   ) %>% -->
<!--   ggplot(aes(x=ground_truth, y=y_rep)) + -->
<!--   geom_line() + -->
<!--   geom_point(data=model_df_hops, aes(x=ground_truth, y=cles), shape = 1, size = 2, color = "royalblue", inherit.aes = FALSE) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   labs( -->
<!--     subtitle = "Posterior predictive distribution for CLES", -->
<!--      ground_truth = NULL, -->
<!--      y_rep = NULL, -->
<!--      cles = NULL -->
<!--   ) + -->
<!--   theme(panel.grid = element_blank()) -->
<!-- ``` -->


##Non-Hierarchical Model of the Probability of Multiple Heuristics in the HOPs Condition

Let's step up the complexity of the model just a little bit by adding in the possibility of additional heuristics. Now, we'll consider as alternative heuristics: the ground truth, outcome proportion, outcome proportion for only the first ten trials, and an ensemble means/spread heuristic. 

Still some important things are missing from this model. The model does not differentiate between workers and focuses on just one visualization condition. We will add in these additional considerations in later iterations.

###Prepare Data List for Modeling in Stan

```{r}
# create data list
data_hops_multi <- list(
  n=length(model_df_hops$trial),                  # total observations (i.e., trials)
  cles=model_df_hops$cles,                        # the worker's response on each trial
  ground_truth=model_df_hops$ground_truth,        # the ground truth cles value on each trial
  n_heuristic=4,                                  # the number of alternative heuristics we model
  n_draws=50,                                     # number of draws displayed in the HOPs per trial
  draws=do.call("rbind",model_df_hops$draws_diff) # trials * draws matrix of the difference between draws B - A shown in HOPs
)
```

###Compile and Run the Model

The model is in the file "stan/hops-only_multi-heuristic.stan". We'll compile the stan code, convert to a model object, and then fit the model.

```{r echo=FALSE}
# compile stan code and create model object
stan_model = stan_model("stan/hops-only_multi-heuristic.stan")
```

```{r}
# fit model
m.hops.multi_heuristic <- sampling(stan_model, data=data_hops_multi, control=list(adapt_delta=0.99), chain=2, cores=2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
traceplot(m.hops.multi_heuristic)
```

- Pairs plot. These look a little skewed again perhaps because sigma and p_heuristic are bounded. The strong correlation between mu_lo_heuristic and p_heuristic is expected since p_heuristic = softmax(mu_lo_heuristic).

```{r}
# pairs plot
pairs(m.hops.multi_heuristic, pars = c('mu_lo_heuristic','sigma','p_heuristic'))
```

- Summary

```{r}
# model summary
print(m.hops.multi_heuristic)
```

Again, the model seems to have learned a fairly even split between heuristics such that the mean of the posteriors for p_heuristic ~= 1 / n_heuristic. Note that these estimates are pretty uncertain.


##Hierarchical Model of the Probability of Multiple Heuristics in the HOPs Condition

To make our model more realistic, we really need to account for the identities of different users so our model can account for individual differences in strategy. Now, we'll allow the set of probabilities for each heuristic to vary from worker-to-worker, and we will learn population level parameters which describe the global pattern of behavior across individuals.

This model still focuses on just one visualization condition (HOPs).

###Prepare Data List for Modeling in Stan

```{r}
# create data list
data_hops_multi_hier <- list(
  n=length(model_df_hops$trial),                         # total observations (i.e., trials)
  cles=model_df_hops$cles,                               # the worker's response on each trial
  ground_truth=model_df_hops$ground_truth,               # the ground truth cles value on each trial
  n_worker=length(unique(model_df_hops$worker_id)),      # number of workers in the sample
  worker=as.integer(as.factor(model_df_hops$worker_id)), # index for worker_id on each trial
  n_heuristic=4,                                         # the number of alternative heuristics we model
  heuristic=1:4,                                         # index for each heuristic
  n_draws=50,                                            # number of draws displayed in the HOPs per trial
  draws=do.call("rbind",model_df_hops$draws_diff)        # trials * draws matrix of the difference between draws B - A shown in HOPs
)
```

###Compile and Run the Model

The model is in the file "stan/hops-only_multi-heuristic-hier.stan". We'll compile the stan code, convert to a model object, and then fit the model.

```{r echo=FALSE}
# compile stan code and create model object
stan_model = stan_model("stan/hops-only_multi-heuristic-hier.stan")
```

```{r}
# fit model
m.hops.multi_heuristic_hier <- sampling(stan_model, data=data_hops_multi_hier, control=list(adapt_delta=0.99, max_treedepth=20), chain=2, cores=2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
traceplot(m.hops.multi_heuristic_hier, pars = c('mu_lo_heuristic','Sigma','sigma_err'))
```

- Pairs plot

```{r}
# pairs plot
pairs(m.hops.multi_heuristic_hier, pars = c('mu_lo_heuristic','sigma_err'))
```

- Summary

```{r}
# model summary
print(m.hops.multi_heuristic_hier)
```

Let's plot the population level estimates for the probability of each heuristic.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.hops.multi_heuristic_hier %>%
  spread_draws(mu_p_heuristic[heuristic]) %>%
  ggplot(aes(y=heuristic, x=mu_p_heuristic)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior estimate of the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

Let's also check posterior predictive distributions for the probability of each heuristic aggregated across trials.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.hops.multi_heuristic_hier %>%
  spread_draws(p_heuristic_hat[1:n,heuristic]) %>%
  ggplot(aes(y=heuristic, x=p_heuristic_hat)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior predictive distribution for the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

It seems like this model is not finding that any one heuristic is more likely than the others overall. Let's check on the estimates for individual subjects.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.hops.multi_heuristic_hier %>%
  spread_draws(p_heuristic[worker, heuristic]) %>%
  ggplot(aes(y=heuristic, x=p_heuristic)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior distribution for the probability of each heuristic per worker"
  ) +
  theme(panel.grid = element_blank()) +
  facet_wrap(. ~ worker)
```

##Running the Hierarchical Model for the HOPs Condition with Fake Data

At this point, we need to understand why p_heuristic is consistently roughly equal to one divided by the number of strategies. One possible reason is that the estimates from our heuristics are too similar for the model to differentiate which pattern has produced the response on a given trial. A second possibility is that our heuristics are misspecified such that they do not account for user behavior and need to be reconsidered. We can differentiate these two possibilities by running our model with fake data. If the model is not successful in recovering fake p_heuristic values when we control the data generating process to match our heuristics, then we know our heuristics are not differentiable by the model. On the other hand, if the model is effective in recovering fake ground truth values for p_heuristic, we will know that the heuristics are differentiable but that they do not adequately capture the data generating process. 

###Simulate Fake Data

Let's start by creating some fake data. We'll simulate 200 trials per worker to see if the model would work with a large amount of data.

```{r}
# parameters to recover
p_heuristic <- c(0.2, 0.4, 0.1, 0.3) # population probabilities of each heuristic
Sigma <- matrix(c(1, .1, .1, .1, .1,  1, .1, .1, .1, .1, 1, .1, .1, .1, .1,  1), 4, 4) # covariance matrix
sigma_err <- 1 # residual error in log odds units

# number of trials per participant (should be a multiple of 20 data conditions)
n_trials_per_worker <- 200
n_trial_scalar <- n_trials_per_worker / 20

# softmax function
softmax <- function(x) {
  return(exp(x) / sum(exp(x)))
}

# heuristic predictions for HOPs data on each trial
heuristics_df_hops <- stimuli_data_df %>% rowwise() %>% 
  mutate( # call heuristic functions
    ground_truth = odds_of_victory * 100,
    outcome_proportion = outcome_proportion(draws_diff),
    outcome_proportion_10 = outcome_proportion(draws_diff[1:10]), # outcome proportion with only the first 10 draws
    ensemble_means_over_uncertainty = means_first_then_uncertainty_hops(draws_diff)
  ) %>% 
  gather(heuristic, heuristic_est, ground_truth, outcome_proportion, outcome_proportion_10, ensemble_means_over_uncertainty) %>% # reshape
  # mutate(ground_truth_rounded = round(odds_of_victory, 3)) %>% # for join
  rename(ground_truth = odds_of_victory) %>%
  select(worker_id, sd_diff, ground_truth, heuristic, heuristic_est)

# calculate p_heuristic per worker (add hierarchy)
fake_worker_params_df <- model_df_hops %>%
  select(worker_id) %>%
  distinct(worker_id) %>%
  rowwise() %>%
  mutate(mu_lo_heuristic_worker = list(MASS::mvrnorm(n(), qlogis(p_heuristic), Sigma))) %>% # intermediate calculation: draws from multivariate normal
  mutate(
    p_heuristic_worker = list(softmax(mu_lo_heuristic_worker)), # what we use to simulate observations for each worker
    mu_lo_heuristic_worker = NULL
  )

# fake data
fake_df_hops <- model_df_hops %>%
  left_join(fake_worker_params_df, by="worker_id") %>%
  slice(rep(1:n(), each=n_trial_scalar)) %>%
  rowwise() %>%
  mutate(heuristic = sample(x=c("ground_truth", "outcome_proportion", "outcome_proportion_10", "ensemble_means_over_uncertainty"), size = n(), replace=TRUE, prob=p_heuristic_worker)) %>% # sample heuristic to use on each trial
  left_join(heuristics_df_hops, by=c("worker_id", "sd_diff", "ground_truth", "heuristic")) %>% # add heuristic estimates to model_df
  select(-cles) %>% # remove actual responses 
  mutate(
    lo_cles = qlogis(heuristic_est / 100) + rnorm(n(), 0, sigma_err), # likelihood function
    cles = plogis(lo_cles) * 100 # simulated responses from known data generating process
  )
```

Now, we prep this data for Stan.

```{r}
# create data list
fakedata_hops_multi_hier <- list(
  n=length(model_df_hops$trial),                         # total observations (i.e., trials)
  cles=model_df_hops$cles,                               # the worker's response on each trial
  ground_truth=model_df_hops$ground_truth,               # the ground truth cles value on each trial
  n_worker=length(unique(model_df_hops$worker_id)),      # number of workers in the sample
  worker=as.integer(as.factor(model_df_hops$worker_id)), # index for worker_id on each trial
  n_heuristic=4,                                         # the number of alternative heuristics we model
  heuristic=1:4,                                         # index for each heuristic
  n_draws=50,                                            # number of draws displayed in the HOPs per trial
  draws=do.call("rbind",model_df_hops$draws_diff)        # trials * draws matrix of the difference between draws B - A shown in HOPs
)
```

###Fit the Model

The model is in the file "stan/hops-only_multi-heuristic-hier.stan". We'll compile the stan code, convert to a model object, and then fit the model.

```{r echo=FALSE}
# compile stan code and create model object
stan_model = stan_model("stan/hops-only_multi-heuristic-hier.stan")
```

```{r}
# fit model
m.fake.hops.multi_heuristic_hier <- sampling(stan_model, data=fakedata_hops_multi_hier, control=list(adapt_delta=0.99, max_treedepth=20), chain=2, cores=2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
traceplot(m.fake.hops.multi_heuristic_hier, pars = c('mu_lo_heuristic','Sigma','sigma_err'))
```

- Pairs plot

```{r}
# pairs plot
pairs(m.fake.hops.multi_heuristic_hier, pars = c('mu_lo_heuristic','sigma_err'))
```

- Summary

```{r}
# model summary
print(m.fake.hops.multi_heuristic_hier)
```

How did we do at recovering the fake values for p_heuristic? Let's plot the population level estimates for the probability of each heuristic to find out.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.fake.hops.multi_heuristic_hier %>%
  spread_draws(mu_p_heuristic[heuristic]) %>%
  ggplot(aes(y=heuristic, x=mu_p_heuristic)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior estimate of the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

Let's also check posterior predictive distributions for the probability of each heuristic aggregated across trials.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.fake.hops.multi_heuristic_hier %>%
  spread_draws(p_heuristic_hat[1:n,heuristic]) %>%
  ggplot(aes(y=heuristic, x=p_heuristic_hat)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior predictive distribution for the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

This does not look super promising. I think our issue might be that we have a model which is a mixture in which the component distributions are mostly overlapping, and we are trying to assign a weight to each component. Given the facts that some of the heuristic estimates (i.e., the components of our mixture) look quite similar and we have a relatively small amount of data, it is unsurprising that our ability to estimate the contribution of each component to the mixture is noisy. I'm guessing we will need more data, more precise heuristics, and perhaps better-inform priors in order for this approach to work.


##Hierarchical Model for the Probility of Multiple Heuristics for Intervals with Means

Maybe the issues we are having are unique to the HOPs condition, where the heuristics we've identified so far don't seem to describe user behavior. However, I doubt that the issue is the precision of the heuristics with regard to the true data generating process since we just tested the model with fake data generated from a process matching the heuristics from the HOPs condition.

Next, let's build a hierarchical model for the intervales with means condition, where the heuristic estimates are similarly overlapping but also better matched with the empirical distribution of worker responses. This model is equivalent to the model in the previous section, but it posits a different set of alternative heuristics which are relevant to intervals with means.

###Prepare Data List for Modeling in Stan

This step looks different than for HOPs since we need different information to calculate the heuristic estimates in this visualization condition.

```{r}
# filter vis condition
model_df_intervals_w_means <- model_df %>% filter(condition=="intervals_w_means")

# calculate axis range for heuristics
data_domain <- c(38, 62)
axis_range <- data_domain[2] - data_domain[1]

# create data list
data_intervals_w_means_multi_hier <- list(
  n=length(model_df_intervals_w_means$trial),                         # total observations (i.e., trials)
  cles=model_df_intervals_w_means$cles,                               # the worker's response on each trial
  ground_truth=model_df_intervals_w_means$ground_truth,               # the ground truth cles value on each trial
  n_worker=length(unique(model_df_intervals_w_means$worker_id)),      # number of workers in the sample
  worker=as.integer(as.factor(model_df_intervals_w_means$worker_id)), # index for worker_id on each trial
  n_heuristic=6,                                                      # the number of alternative heuristics we model
  heuristic=1:6,
  # necessary information to compute heuristic estimates
  mean_diff=model_df_intervals_w_means$mean_diff,
  max_abs_mean_diff=unique(model_df_intervals_w_means$max_abs_mean_diff),
  axis_range=axis_range,
  sd_team=model_df_intervals_w_means$sd
)
```

###Compile and Run the Model

The model is in the file "stan/intervals-w-means-only_multi-heuristic-hier.stan". We'll compile the stan code, convert to a model object, and then fit the model.

```{r echo=FALSE}
# compile stan code and create model object
stan_model = stan_model("stan/intervals-w-means-only_multi-heuristic-hier.stan")
```

```{r}
# fit model
m.intervals_w_means.multi_heuristic_hier <- sampling(stan_model, data=data_intervals_w_means_multi_hier, control=list(adapt_delta=0.99, max_treedepth=20), chain=2, cores=2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
traceplot(m.intervals_w_means.multi_heuristic_hier, pars = c('mu_lo_heuristic','Sigma','sigma_err'))
```

- Pairs plot

```{r}
# pairs plot
pairs(m.intervals_w_means.multi_heuristic_hier, pars = c('mu_lo_heuristic','sigma_err'))
```

- Summary

```{r}
# model summary
print(m.intervals_w_means.multi_heuristic_hier)
```

Let's plot the population level estimates for the probability of each heuristic.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.intervals_w_means.multi_heuristic_hier %>%
  spread_draws(mu_p_heuristic[heuristic]) %>%
  ggplot(aes(y=heuristic, x=mu_p_heuristic)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior estimate of the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

Let's also check posterior predictive distributions for the probability of each heuristic aggregated across trials.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.intervals_w_means.multi_heuristic_hier %>%
  spread_draws(p_heuristic_hat[1:n,heuristic]) %>%
  ggplot(aes(y=heuristic, x=p_heuristic_hat)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior predictive distribution for the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

Again, it looks like our model is unable to differentiate the probability of each heuristic. Let's check on the estimates for individual subjects.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.intervals_w_means.multi_heuristic_hier %>%
  spread_draws(p_heuristic[worker, heuristic]) %>%
  ggplot(aes(y=heuristic, x=p_heuristic)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior distribution for the probability of each heuristic per worker"
  ) +
  theme(panel.grid = element_blank()) +
  facet_wrap(. ~ worker)
```


##Running the Hierarchical Model for Fake Data Under Ideal Circumstances

At this point, it seems very likely that the reason our model is not differentiating the probabilities of alternative heuristics has to do with the fact that the heuristic estimates are highly overlapping, making it difficult for the model to say which heuristic submodel is more likely to have produced a given data point. This should not be the case when heuristics are sufficiently different in the responses they predict.

Let's see if the model works for a mixture of two very different distributions which should be separable when we have plenty of observations per participant.

###Simulate Fake Data

Let's start by creating some fake data with many trials per participant. We'll create the data as a mixture between the ground truth and a strategy of responding that CLES is 100% on every trial.

```{r}
# parameters to recover
p_heuristic <- c(0.2, 0.8) # population probabilities of each heuristic
Sigma <- matrix(c(1, .1, .1, 1), 2, 2) # covariance matrix
sigma_err <- 1 # residual error in log odds units

# number of trials per participant (should be a multiple of 20 data conditions)
n_trials_per_worker <- 200
n_trial_scalar <- n_trials_per_worker / 20

# softmax function
softmax <- function(x) {
  return(exp(x) / sum(exp(x)))
}

# heuristic predictions on each trial: ground_truth vs guess_one
heuristics_df_hops <- stimuli_data_df %>% rowwise() %>% 
  mutate( # heuristic functions
    ground_truth = odds_of_victory * 100,
    guess_one = 99.75
  ) %>% 
  gather(heuristic, heuristic_est, ground_truth, guess_one) %>% # reshape
  rename(ground_truth = odds_of_victory) %>%
  select(worker_id, sd_diff, ground_truth, heuristic, heuristic_est)

# calculate p_heuristic per worker (add hierarchy)
fake_worker_params_df <- model_df %>%
  select(worker_id) %>%
  distinct(worker_id) %>%
  rowwise() %>%
  mutate(p_heuristic_worker = list(p_heuristic)) # no hierarchy for p_heuristic values
  # mutate(mu_lo_heuristic_worker = list(MASS::mvrnorm(n(), qlogis(p_heuristic), Sigma))) %>% # intermediate calculation: draws from multivariate normal
  # mutate(
  #   p_heuristic_worker = list(softmax(mu_lo_heuristic_worker)), # what we use to simulate observations for each worker
  #   mu_lo_heuristic_worker = NULL
  # )

# fake data
fake_df_ideal <- model_df %>%
  left_join(fake_worker_params_df, by="worker_id") %>%
  slice(rep(1:n(), each=n_trial_scalar)) %>%
  rowwise() %>%
  mutate(heuristic = sample(x=c("ground_truth", "guess_one"), size = n(), replace=TRUE, prob=p_heuristic_worker)) %>% # sample heuristic to use on each trial
  left_join(heuristics_df_hops, by=c("worker_id", "sd_diff", "ground_truth", "heuristic")) %>% # add heuristic estimates to model_df
  select(-cles) %>% # remove actual responses 
  mutate(
    lo_cles = qlogis(heuristic_est / 100) + rnorm(n(), 0, sigma_err), # likelihood function
    cles = plogis(lo_cles) * 100 # simulated responses from known data generating process
  )
```

Now, we prep this data for Stan.

```{r}
# create data list
fakedata_ideal_multi_hier <- list(
  n=length(fake_df_ideal$trial),                         # total observations (i.e., trials)
  cles=fake_df_ideal$cles,                               # the worker's response on each trial
  ground_truth=fake_df_ideal$ground_truth,               # the ground truth cles value on each trial
  n_worker=length(unique(fake_df_ideal$worker_id)),      # number of workers in the sample
  worker=as.integer(as.factor(fake_df_ideal$worker_id)), # index for worker_id on each trial
  n_heuristic=2,                                         # the number of alternative heuristics we model
  heuristic=1:2                                         # index for each heuristic
)
```

###Fit the Model

The model is in the file "stan/ideal_multi-heuristic-hier.stan". We'll compile the stan code, convert to a model object, and then fit the model.

```{r echo=FALSE}
# compile stan code and create model object
stan_model = stan_model("stan/ideal_multi-heuristic-hier.stan")
```

```{r}
# fit model
m.fake.ideal.multi_heuristic_hier <- sampling(stan_model, data=fakedata_ideal_multi_hier, control=list(adapt_delta=0.99, max_treedepth=20), chain=2, cores=2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
traceplot(m.fake.ideal.multi_heuristic_hier, pars = c('mu_lo_heuristic','Sigma','sigma_err'))
```

- Pairs plot

```{r}
# pairs plot
pairs(m.fake.ideal.multi_heuristic_hier, pars = c('mu_lo_heuristic','sigma_err'))
```

- Summary

```{r}
# model summary
print(m.fake.ideal.multi_heuristic_hier)
```

How did we do at recovering the fake values for p_heuristic? Let's plot the population level estimates for the probability of each heuristic to find out.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.fake.ideal.multi_heuristic_hier %>%
  spread_draws(mu_p_heuristic[heuristic]) %>%
  ggplot(aes(y=heuristic, x=mu_p_heuristic)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior estimate of the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

Let's also check posterior predictive distributions for the probability of each heuristic aggregated across trials.

```{r}
# plot transformed posterior predictions for probability of each heuristic
m.fake.ideal.multi_heuristic_hier %>%
  spread_draws(p_heuristic_hat[1:n,heuristic]) %>%
  ggplot(aes(y=heuristic, x=p_heuristic_hat)) +
  geom_halfeyeh() + 
  labs(
    subtitle = "Posterior predictive distribution for the probability of each heuristic"
  ) +
  theme(panel.grid = element_blank())
```

For reference let's look at the distribution of the simulated data.

```{r}
fake_df_ideal %>% ggplot(aes(x = qlogis(ground_truth), y = qlogis(cles / 100))) +
  geom_point(alpha=0.2) +
  labs(subtitle = "Simulated responses vs ground truth") +
  theme(panel.grid = element_blank())
```



#Linear Log Odds Model (Plan B)

To get a sense of what we can infer from the data, we fit a linear model of CLES judgments vs the ground truth in log odds units. This model follows from [related work](https://www.frontiersin.org/articles/10.3389/fnins.2012.00001/full) suggesting that the human perception of probability is encoded on a log odds scale. On this scale, the slope of a linear model represents the shape and severity of the function describing bias in probability perception. The greater the deviation of from a slope of 1 (i.e., ideal performance), the more biased the judgments of probability. Slopes less than one correspond to the kind of bias predicted by excessive attention to the mean. On the same log odds scale, the intercept is a crossover-point which should be proportional to the number of categories of possible outcomes among which probability is divided. In our case, the intercept should be about 0.5 since workers are judging the probability of team A vs team B winning a round of a simulated game.

We'll use the brms package for this model since it is relatively more orthodox than our models of the probability of alternative heuristics, and we do not really need to customize our likelihood function.


##Prepare Data for Modeling

Convert response and ground truth units to log odds.

```{r}
# apply logit function to cles judgments and ground truth
model_df_llo <- model_df %>%
  mutate(
    lo_cles=qlogis(cles / 100),
    lo_ground_truth=qlogis(ground_truth),
    sd_diff=as.factor(sd_diff)
  )
```


##Distribution of CLES

We start as simple as possible by just modeling the distribution of CLES on the log odds scale.

Let's check that our priors seem reasonable.

```{r}
# prior predictive check
n <- 1e4
tibble(sample_mu    = rnorm(n, mean = 0, sd = 1),
       sample_sigma = rnorm(n, mean = 0, sd = 1)) %>% 
  mutate(lo_cles = rnorm(n, mean = sample_mu, sd = sample_sigma),
         cles = plogis(lo_cles)) %>% 
  ggplot(aes(x = cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression(paste("Prior predictive distribution for ", italic(h[i]))),
       lo_cles = NULL) +
  theme(panel.grid = element_blank())
```

Fit an intercept model.

```{r}
# starting as simple as possible: learn the distribution of lo_cles
m.lo_cles <- brm(data = model_df_llo, family = gaussian,
              lo_cles ~ 1,
              prior = c(prior(normal(0, 1), class = Intercept),
                        prior(normal(0, 1), class = sigma)),
              iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Check diagnostics:

- Trace plots. The value of sigma seems pretty large.

```{r}
# trace plots
plot(m.lo_cles)
```

- Pairs plot. These look great.

```{r}
# pairs plot
pairs(m.lo_cles)
```

- Summary

```{r}
# model summary
print(m.lo_cles)
```

- Posterior draws

```{r}
# get posterior draws
post <- posterior_samples(m.lo_cles)

# get summary stats on distribution of each parameter
post %>% 
  select(-lp__) %>% # drop log probability of posterior draws
  gather(parameter) %>%
  group_by(parameter) %>%
  mean_qi(value)
```

- Posterior predictive distribution. This isn't working because exp(post_lo_cles) => inf.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth) %>% # not used
  add_predicted_draws(m.lo_cles, prediction = "lo_cles", seed = 1234) %>% # get draws from fitted model
  mutate(post_cles = plogis(lo_cles)) %>% # transform
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```


##Linear Log Odds Model of CLES

Now well add in a slope parameter. Hopefully, this will reduce the residual variance.

Let's check that our priors seem reasonable.

```{r}
# prior predictive check
n <- 1e4
tibble(intercept    = rnorm(n, mean = 0, sd = 1),
       slope        = rnorm(n, mean = 0, sd = 1),
       sample_sigma = rnorm(n, mean = 0, sd = 1),
       lo_ground_truth = list(qlogis(unique(stimuli_data_df$odds_of_victory)))) %>% 
  unnest() %>%
  mutate(lo_cles = rnorm(n * length(unique(stimuli_data_df$odds_of_victory)), mean = intercept + slope * lo_ground_truth, sd = sample_sigma),
         cles = plogis(lo_cles)) %>% 
  ggplot(aes(x = cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression(paste("Prior predictive distribution for ", italic(h[i]))),
       lo_cles = NULL) +
  theme(panel.grid = element_blank())
```

What we get using generic weakly informative priors seems reasonable given that we are sampling more at extreme levels of ground truth than values in the middle of the scale.

Now, let's fit the LLO model.

```{r}
# linear log odds model: lo_cles ~ 1 + lo_ground_truth
m.llo_cles <- brm(data = model_df_llo, family = gaussian,
              lo_cles ~ 1 + lo_ground_truth,
              prior = c(prior(normal(0, 1), class = Intercept),
                        prior(normal(0, 1), class = b),
                        prior(normal(0, 1), class = sigma)),
              iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.llo_cles)
```

- Pairs plot. It looks like we might have an issue with multicolinearity because b_Intercept and b_lo_ground_truth are so highly correlated.

```{r}
# pairs plot
pairs(m.llo_cles)
```

- Summary

```{r}
# model summary
print(m.llo_cles)
```

Let's check out a posterior predictive distribution for CLES. 

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth) %>%
  add_predicted_draws(m.llo_cles, prediction = "lo_cles", seed = 1234) %>%
  mutate(post_cles = plogis(lo_cles)) %>%
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

The posterior predictive distributions seems too wide. How does the posterior predictions compare to the observed data?

```{r}
# data density
model_df_llo %>%
  ggplot(aes(x=cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for CLES",
       cles = NULL) +
  theme(panel.grid = element_blank())
```

This seems to suggest that the data generating process is a 50% inflated mixture.

Let's take a look at some of the estimated linear models.

```{r}
# get posterior samples
post  <- posterior_samples(m.llo_cles)

# plot estimated linear models against observed data
model_df_llo %>%
ggplot(aes(x = lo_ground_truth, y = lo_cles)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  geom_abline(intercept = post[1:20, 1],
              slope     = post[1:20, 2],
              size = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_cles, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank())
```


##Add Different Linear Models per Visualization Condition

In the LLO framework, what we really want to know about is the impact of visualization condition on the slopes of linear models in log odds space. Do some visualizations lead to more extreme patterns of bias than others? To test this, we'll add an interaction between visualization condition and the ground truth.

```{r}
# update the llo model of cles responses to include an interaction
m.vis.llo_cles <- update(m.llo_cles, 
                         # formula = lo_cles ~ 1 + lo_ground_truth + condition + lo_ground_truth:condition,
                         formula = lo_cles ~ 1 + lo_ground_truth + lo_ground_truth:condition,
                         newdata = model_df_llo)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.vis.llo_cles)
```

- Pairs plot

```{r}
# pairs plot
pairs(m.vis.llo_cles)
```

- Summary

```{r}
# model summary
print(m.vis.llo_cles)
```

Let's check out a posterior predictive distribution for CLES. This should look more like a mixture with different slopes, but it still looks really wide.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition) %>%
  add_predicted_draws(m.vis.llo_cles, prediction = "lo_cles", seed = 1234) %>%
  mutate(post_cles = plogis(lo_cles)) %>%
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

What do the posterior for the effect of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.vis.llo_cles) %>%
  transmute(slope_HOPs = b_lo_ground_truth,
            slope_intervals_w_means = b_lo_ground_truth + `b_lo_ground_truth:conditionintervals_w_means`,
            slope_means_only = b_lo_ground_truth + `b_lo_ground_truth:conditionmeans_only`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

This suggests that users are more biased toward responses of 50% when the mean is visually salient.

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# set up new dataframe for prediction
nd <- tibble(lo_ground_truth = seq(from = quantile(model_df_llo$lo_ground_truth, 0), to = quantile(model_df_llo$lo_ground_truth, 1), length.out = 30) %>% 
         rep(., times = 3),
         condition = rep(unique(model_df_llo$condition), each = 30))

# pass our new predictive dataframe through our fitted model
fitd_m.vis.llo_cles <- fitted(m.vis.llo_cles, newdata = nd) %>%
  as_tibble() %>%
  bind_cols(nd)
```

```{r}
# plot estimated linear models against observed data
model_df_llo %>%
  ggplot(aes(x = lo_ground_truth)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  geom_ribbon(data = fitd_m.vis.llo_cles,
              aes(ymin  = Q2.5, 
                  ymax  = Q97.5,
                  fill  = condition,
                  group = condition),
              alpha = .25) +
  geom_line(data = fitd_m.vis.llo_cles,
              aes(y     = Estimate, 
                  color = condition,
                  group = condition)) +
  geom_point(aes(y = lo_cles, color = condition)) +
    scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_cles, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  facet_grid(. ~ condition)
```

This looks pretty promising, but let's see if we can parse some of the variability into error vs individual variability in slopes.


##Add Hierarchy for Slopes

In the LLO framework, what we really want to know about is the impact of visualization condition on the slopes of linear models in log odds space. Do some visualizations lead to more extreme patterns of bias than others? To test this, we'll add an interaction between visualization condition and the ground truth.

```{r}
# update the llo model of cles responses to include an interaction
m.vis.wrkr.llo_cles <- update(m.llo_cles, 
                         # formula = lo_cles ~ 1 + (lo_ground_truth|worker_id) + condition + lo_ground_truth:condition,
                         formula = lo_cles ~ 1 + (lo_ground_truth|worker_id) + lo_ground_truth:condition,
                         newdata = model_df_llo)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.vis.wrkr.llo_cles)
```

- Pairs plot

```{r}
# pairs plot
pairs(m.vis.wrkr.llo_cles)
```

- Summary

```{r}
# model summary
print(m.vis.wrkr.llo_cles)
```

Let's check out a posterior predictive distribution for CLES. This distribution is looking narrower than in previous iterations of the model, which suggests that individual variation in slopes accounts for some (but not all) of the 50% responses.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition,worker_id) %>%
  add_predicted_draws(m.vis.wrkr.llo_cles, prediction = "lo_cles", seed = 1234) %>%
  mutate(post_cles = plogis(lo_cles)) %>%
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

What do the posterior for the effect of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.vis.wrkr.llo_cles) %>%
  # transmute(slope_HOPs = `b_conditionHOPs:lo_ground_truth`,
  #           slope_intervals_w_means = `b_conditionintervals_w_means:lo_ground_truth`,
  #           slope_means_only = `b_conditionmeans_only:lo_ground_truth`) %>%
  transmute(slope_HOPs = `b_lo_ground_truth:conditionHOPs`,
            slope_intervals_w_means = `b_lo_ground_truth:conditionintervals_w_means`,
            slope_means_only = `b_lo_ground_truth:conditionmeans_only`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

The effect we saw earlier is still present.

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# this time we'll adopt functions from the tidybayes package to make plotting posterior predictions easier
model_df_llo %>%
  group_by(condition,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.vis.wrkr.llo_cles) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_cles, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_cles, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
```

What does this look like in probability units?

```{r}
# this time we'll adopt functions from the tidybayes package to make plotting posterior predictions easier
model_df_llo %>%
  group_by(condition,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.vis.wrkr.llo_cles) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_cles), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(plogis(model_df_llo$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df_llo$lo_cles), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
```

This is very promising, but there are areas of high posterior density in the interals_w_means and means_only conditions where there are no observations. Maybe adding the level of uncertainty shown in each plot to the model will improve the fit?


##Add an Interaction Between the Level of Uncertainty in the Data and Slopes 

```{r}
# update the llo model of cles responses to include an interaction
m.vis.sd.wrkr.llo_cles <- update(m.llo_cles, 
                         formula = lo_cles ~ 1 + (lo_ground_truth|worker_id) + lo_ground_truth:condition:sd_diff,
                         newdata = model_df_llo)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.vis.sd.wrkr.llo_cles)
```

- Pairs plot

```{r}
# pairs plot
pairs(m.vis.sd.wrkr.llo_cles)
```

- Summary

```{r}
# model summary
print(m.vis.sd.wrkr.llo_cles)
```

Let's check out a posterior predictive distribution for CLES. This still looks too wide considering the number of 50% responses we see in the data.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition,sd_diff,worker_id) %>%
  add_predicted_draws(m.vis.sd.wrkr.llo_cles, prediction = "lo_cles", seed = 1234) %>%
  mutate(post_cles = plogis(lo_cles)) %>%
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

What do the posterior for the effect of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.vis.sd.wrkr.llo_cles) %>%
  transmute(slope_HOPs = `b_lo_ground_truth:conditionHOPs:sd_diff1` + `b_lo_ground_truth:conditionHOPs:sd_diff5`,
            slope_intervals_w_means = `b_lo_ground_truth:conditionintervals_w_means:sd_diff1` + `b_lo_ground_truth:conditionintervals_w_means:sd_diff5`,
            slope_means_only = `b_lo_ground_truth:conditionmeans_only:sd_diff1` + `b_lo_ground_truth:conditionmeans_only:sd_diff5`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# this time we'll adopt functions from the tidybayes package to make plotting posterior predictions easier
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.vis.sd.wrkr.llo_cles) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_cles, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_cles, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
```

What does this look like in probability units?

```{r}
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.vis.sd.wrkr.llo_cles) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_cles), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(plogis(model_df_llo$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df_llo$lo_cles), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(sd_diff ~ condition)
```

This adding an interaction for sd_diff doesn't really fix the problem that our posterior predictions have high density where there are few responses. However it does clarify that the differences between visualization conditions are more pronounced when sd_diff is low.


##A Mixture Model of Fake CLES Responses

In order to help our model accommodate the fact that some workers choose the same response regardless of the ground truth, we will need a mixture between two submodels: a LLO process vs a constant response process. To get this working, we'll start with our ideal fake data from earlier.

###Fake Data

This simulated data is a 20/80 mixture of the ground truth vs a constant response of 99.75%, both with standard normal noise added. Let's prep the data for the model.

```{r}
# prep data
fake_df_llo_mix <- fake_df_ideal %>%
  select(worker_id,sd_diff,ground_truth,condition,cles) %>%
  mutate(
    lo_cles=qlogis(cles / 100),
    lo_ground_truth=qlogis(ground_truth)
  )
```

We fit a model that matches the data-generating process, and importantly, we estimate the log odds of the constant response strategy theta2. Estimating the proportion of each submodel is exactly what we were struggling with in our Stan model of specific heuristics.

```{r}
# get_prior(
#   bf(lo_cles ~ 1, mu1 ~ (lo_ground_truth|worker_id), mu2 ~ (1|worker_id), theta2 ~ 1),
#     data = fake_df_llo_mix,
#     family = mixture(gaussian, gaussian)
# )

# fit the model
m.fake.llo_mix <- brm(
  bf(lo_cles ~ 1, mu1 ~ lo_ground_truth, mu2 ~ 1, theta2 ~ 1),
  data = fake_df_llo_mix,
  family = mixture(gaussian, gaussian, order = 'mu'),
  prior = c(
    prior(normal(0, 1), class = Intercept, dpar = mu1),
    prior(normal(0, 1), class = Intercept, dpar = mu2)
    # prior(beta(2, 2), class = Intercept, dpar = theta2) # default prior seems to be in log odds units
  ),
  inits = 1, chains = 1, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=12)
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.fake.llo_mix)
```

- Pairs plot

```{r}
# pairs plot
pairs(m.fake.llo_mix)
```

- Summary

```{r}
# model summary
print(m.fake.llo_mix)
```

So, did we accurately recover the mixture proportions? Let's find out by plotting the posterior for theta. Because theta is in log odds units we'll transform it into probability units.

```{r}
# posteriors of mixture proportion
posterior_samples(m.fake.llo_mix) %>%
  mutate(p_mix = plogis(b_theta2_Intercept)) %>%
  ggplot(aes(x=p_mix)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior distribution for mixture proportion") +
  theme(panel.grid = element_blank())
```

Yes, we did it! The ability to estimate the proportion of each alternative within a mixture should help us to dramatically improve our models of real data.

Let's also check out a posterior predictive distribution for the fake CLES responses.

```{r}
# posterior predictive check
fake_df_llo_mix %>%
  select(lo_ground_truth,condition,sd_diff,worker_id) %>%
  add_predicted_draws(m.fake.llo_mix, prediction = "lo_cles", seed = 1234) %>%
  mutate(post_cles = plogis(lo_cles)) %>%
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

And lets's compare this to the data distribution.

```{r}
# posterior predictive check
fake_df_llo_mix %>%
  ggplot(aes(x=cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Distribution of fake CLES responses",
       cles = NULL) +
  theme(panel.grid = element_blank())
```


##A Mixture of the LLO Model vs A Random Constant Response

Let's adopt this mixture model for our actual CLES responses.

```{r}
m.cles.llo_mix <- brm(
  bf(lo_cles ~ 1, 
    mu1 ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth:condition:sd_diff, # our most recent llo model
    mu2 ~ (1|worker_id), # random constant response per worker (to account for people who always answer the same, often but not always 50%)
    theta2 ~ (1|worker_id) + condition # the proportion of responses that are constant
  ),
  data = model_df_llo,
  family = mixture(gaussian, gaussian, order = 'mu'),
  prior = c(
    prior(normal(0, 1), class = Intercept, dpar = mu1),
    prior(normal(0, 1), class = Intercept, dpar = mu2)
  ),
  inits = 1, chains = 2, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=15),
  file = "stan/m_cles_llo_mix"
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.cles.llo_mix)
```

- Pairs plot

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# mixture proportions
pairs(m.cles.llo_mix, pars = c("b_theta2_Intercept",
                               "sd_worker_id__theta2_Intercept",
                               "b_theta2_conditionHOPs",
                               "b_theta2_conditionmeans_only",
                               "b_theta2_conditionintervals_w_means"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# hyperparameters
pairs(m.cles.llo_mix, pars = c("b_mu1_Intercept",
                               "sd_worker_id__mu1_Intercept",
                               "sd_worker_id__mu1_lo_ground_truth",
                               "cor_worker_id__mu1_Intercept__mu1_lo_ground_truth",
                               "sigma1",
                               "b_mu2_Intercept",
                               "sd_worker_id__mu2_Intercept",
                               "sigma2"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# slope effects
pairs(m.cles.llo_mix, pars = c("b_mu1_lo_ground_truth:conditionHOPs:sd_diff1",
                               "b_mu1_lo_ground_truth:conditionintervals_w_means:sd_diff1",
                               "b_mu1_lo_ground_truth:conditionmeans_only:sd_diff1",
                               "b_mu1_lo_ground_truth:conditionHOPs:sd_diff5",
                               "b_mu1_lo_ground_truth:conditionintervals_w_means:sd_diff5",
                               "b_mu1_lo_ground_truth:conditionmeans_only:sd_diff5"))
```

- Summary

```{r}
# model summary
print(m.cles.llo_mix)
```

There's a couple problems with this model:
- First, the posterior samples are highly correlated among the estimates of the prevalence of the constant response strategy in each visualization condition. It is clear to me that these three parameters should have a multivariate normal prior (i.e., the model should know about this correlation between conditions), so we'll try that next.
- The second issue I'm seeing in the pairs plots is that the interaction effects for different levels of sd_diff within the same visualization condition are highly correlated. I'm wondering if this means I should remove sd_diff from the model? We'll try this as well. Based on the performance of the model below, I'm thinking that the submodel representing a constant response pattern is redundant with including sd_diff in the llo model insofar as both account for the high frequency of responses near 50%, but the mixture seems to be a better match to the data-generating process based on the posterior density of the mixture model.


##Revised Mixture of the LLO Model vs A Random Constant Response

This is an adaptation of the previous model which attempts to remedy the issues we saw in the pairs plots.

```{r}
# define stanvars for multi_normal prior on condition effects
stanvars <- stanvar(rep(1, 3), "mu_theta2", scode = "  vector[3] mu_theta2;") +
  stanvar(diag(3), "Sigma_theta2", scode = "  matrix[3, 3] Sigma_theta2;")
# fit the model
m.cles.llo_mix <- brm(
   bf(lo_cles ~ 1, 
    mu1 ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth:condition, # our most recent llo model
    mu2 ~ (1|worker_id), # random constant response per worker (to account for people who always answer the same, often but not always 50%)
    theta2 ~ (1|worker_id) + 0 + condition # the proportion of responses that are constant
  ),
  data = model_df_llo,
  family = mixture(gaussian, gaussian, order = 'mu'),
  prior = c(
    prior(normal(0, 1), class = Intercept, dpar = mu1),
    prior(normal(0, 1), class = Intercept, dpar = mu2),
    prior("multi_normal(mu_theta2, Sigma_theta2)", class = b, dpar = theta2)
  ),
  stanvars = stanvars,
  inits = 1, chains = 2, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=15),
  file = "stan/m_cles_llo_mix4"
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.cles.llo_mix)
```

- Pairs plot

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# mixture proportions
pairs(m.cles.llo_mix, pars = c("sd_worker_id__theta2_Intercept",
                               "b_theta2_conditionHOPs",
                               "b_theta2_conditionmeans_only",
                               "b_theta2_conditionintervals_w_means"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# hyperparameters
pairs(m.cles.llo_mix, pars = c("b_mu1_Intercept",
                               "sd_worker_id__mu1_Intercept",
                               "sd_worker_id__mu1_lo_ground_truth",
                               "cor_worker_id__mu1_Intercept__mu1_lo_ground_truth",
                               "sigma1",
                               "b_mu2_Intercept",
                               "sd_worker_id__mu2_Intercept",
                               "sigma2"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# slope effects
pairs(m.cles.llo_mix, pars = c("b_mu1_lo_ground_truth:conditionHOPs",
                               "b_mu1_lo_ground_truth:conditionmeans_only",
                               "b_mu1_lo_ground_truth:conditionintervals_w_means"))
```

- Summary

```{r}
# model summary
print(m.cles.llo_mix)
```

Let's check out a posterior predictive distribution for CLES.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition,sd_diff,worker_id) %>%
  add_predicted_draws(m.cles.llo_mix, prediction = "lo_cles", seed = 1234) %>%
  mutate(post_cles = plogis(lo_cles)) %>%
  ggplot(aes(x=post_cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

How does this compare to the empirical distribution of CLES responses?

```{r}
# posterior predictive check
model_df_llo %>%
  ggplot(aes(x=cles)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for CLES",
       post_cles = NULL) +
  theme(panel.grid = element_blank())
```

What do the posterior for the effect of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.cles.llo_mix) %>%
  transmute(slope_HOPs = `b_mu1_lo_ground_truth:conditionHOPs`,
            slope_intervals_w_means = `b_mu1_lo_ground_truth:conditionintervals_w_means`,
            slope_means_only = `b_mu1_lo_ground_truth:conditionmeans_only`) %>%
  # transmute(slope_HOPs = `b_mu1_lo_ground_truth:conditionHOPs:sd_diff1` + `b_mu1_lo_ground_truth:conditionHOPs:sd_diff5`,
  #           slope_intervals_w_means = `b_mu1_lo_ground_truth:conditionintervals_w_means:sd_diff1` + `b_mu1_lo_ground_truth:conditionintervals_w_means:sd_diff5`,
  #           slope_means_only = `b_mu1_lo_ground_truth:conditionmeans_only:sd_diff1` + `b_mu1_lo_ground_truth:conditionmeans_only:sd_diff5`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# this time we'll adopt functions from the tidybayes package to make plotting posterior predictions easier
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.cles.llo_mix) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_cles, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_cles, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  facet_grid(. ~ condition)
  # facet_grid(sd_diff ~ condition)
```

What does this look like in probability units?

```{r}
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.cles.llo_mix) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_cles), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(plogis(model_df_llo$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df_llo$lo_cles), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
  # facet_grid(sd_diff ~ condition)
```

What about the mixture proportions? Let's plot the posterior for theta. Because theta is in log odds units we'll transform it into probability units.

```{r}
# posteriors of mixture proportion
posterior_samples(m.cles.llo_mix) %>%
  transmute(#p_mix_HOPs = plogis(b_theta2_Intercept),
            p_mix_HOPs = plogis(b_theta2_conditionHOPs),
            p_mix_intervals_w_means = plogis(b_theta2_conditionintervals_w_means),
            p_mix_means_only = plogis(b_theta2_conditionmeans_only)) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for proportion of constant response by visualization condition") +
  theme(panel.grid = element_blank())
```

All of this looks good! Let's move on to modeling the betting responses.


#Model Betting Behavior

Recall that in the betting data we see a set of different betting strategies. Some workers bet the minimum, maximum, or middle amount every trial. This behavior is captured by a constant response strategy. Very few workers seem to employ the optimal betting strategy where the bet amount is a linear function of the probability of winning. Since the perception of probability is biased (i.e., an inverted sigmoid pi function), responses in this strategy should look like CLES judgments but maybe noisier. Others seem to have some kind of probability threshold where they bet the minimum amount below that threshold and the maximum amount above that threshold (i.e., a sigmoid psychometric function). Both the optimal betting strategy and the threshold strategy can be parameterized within the LLO frame work because the shape of the LLO model is sigmoidal when slope > 1 and an inverse sigmoid where slope < 1. Thus, the patterns we observer in betting behavior should be accounted for by a mixture between LLO and constant response submodels. 

Thankfully, we've already build such a model for our CLES responses.

##Fit the LLO vs Constant Response Mixure to Betting Data

###Prep Data

We prep the betting responses for the model by treating them as proportions of the budget amount and converting to log odds units.

```{r}
# apply logit function to bet amounts, treating them as probability judgments on a scale from 1 to 1000
model_df_llo <- model_df_llo %>%
  mutate(
    lo_bet=qlogis(bet / 1000)
  )
```

###Fit Model

```{r}
# define stanvars for multi_normal prior on condition effects
stanvars <- stanvar(rep(1, 3), "mu_theta2", scode = "  vector[3] mu_theta2;") +
  stanvar(diag(3), "Sigma_theta2", scode = "  matrix[3, 3] Sigma_theta2;")
# fit the model
m.bet.llo_mix <- brm(
   bf(lo_bet ~ 1, 
    mu1 ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth:condition, # llo model
    mu2 ~ (1|worker_id), # random constant response per worker (to account for people who always answer the same, often but not always 50%)
    theta2 ~ (1|worker_id) + 0 + condition # the proportion of responses that are constant in each condition
  ),
  data = model_df_llo,
  family = mixture(gaussian, gaussian, order = 'mu'),
  prior = c(
    prior(normal(0, 1), class = Intercept, dpar = mu1),
    prior(normal(0, 1), class = Intercept, dpar = mu2),
    prior("multi_normal(mu_theta2, Sigma_theta2)", class = b, dpar = theta2)
  ),
  stanvars = stanvars,
  inits = 1, chains = 2, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=15),
  file = "stan/m_bet_llo_mix"
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.bet.llo_mix)
```

- Pairs plot

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# mixture proportions
pairs(m.bet.llo_mix, pars = c("sd_worker_id__theta2_Intercept",
                              "b_theta2_conditionHOPs",
                              "b_theta2_conditionmeans_only",
                              "b_theta2_conditionintervals_w_means"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# hyperparameters
pairs(m.bet.llo_mix, pars = c("b_mu1_Intercept",
                               "sd_worker_id__mu1_Intercept",
                               "sd_worker_id__mu1_lo_ground_truth",
                               "cor_worker_id__mu1_Intercept__mu1_lo_ground_truth",
                               "sigma1",
                               "b_mu2_Intercept",
                               "sd_worker_id__mu2_Intercept",
                               "sigma2"
                              ))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# slope effects
pairs(m.bet.llo_mix, pars = c("b_mu1_lo_ground_truth:conditionHOPs",
                               "b_mu1_lo_ground_truth:conditionmeans_only",
                               "b_mu1_lo_ground_truth:conditionintervals_w_means"))
```

- Summary

```{r}
# model summary
print(m.bet.llo_mix)
```

Let's check out a posterior predictive distribution for bets.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition,sd_diff,worker_id) %>%
  add_predicted_draws(m.bet.llo_mix, prediction = "lo_bet", seed = 1234) %>%
  mutate(post_bet = plogis(lo_bet)) %>%
  ggplot(aes(x=post_bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for bets") +
  theme(panel.grid = element_blank())
```

How does this compare to the actual distribution of bet amounts?

```{r}
# posterior predictive check
model_df_llo %>%
  ggplot(aes(x=bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Empirical distribution of bets") +
  theme(panel.grid = element_blank())
```

What do the posterior for the effect of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.bet.llo_mix) %>%
  transmute(slope_HOPs = `b_mu1_lo_ground_truth:conditionHOPs`,
            slope_intervals_w_means = `b_mu1_lo_ground_truth:conditionintervals_w_means`,
            slope_means_only = `b_mu1_lo_ground_truth:conditionmeans_only`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# this time we'll adopt functions from the tidybayes package to make plotting posterior predictions easier
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.bet.llo_mix) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_bet, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo, alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_bet, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(sd_diff ~ condition)
```

What does this look like in probability units?

```{r}
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.bet.llo_mix) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_bet), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo, alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(plogis(model_df_llo$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df_llo$lo_bet), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(sd_diff ~ condition)
```

What about the mixture proportions? Let's plot the posterior for theta. Because theta is in log odds units we'll transform it into probability units.

```{r}
# posteriors of mixture proportion
posterior_samples(m.bet.llo_mix) %>%
  transmute(p_mix_HOPs = plogis(b_theta2_conditionHOPs),
            p_mix_intervals_w_means = plogis(b_theta2_conditionintervals_w_means),
            p_mix_means_only = plogis(b_theta2_conditionmeans_only)) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for proportion of constant response by visualization condition") +
  theme(panel.grid = element_blank())
```

This model has all sorts of issues for our betting data, so let's revert to something simpler.


##Back to the LLO Model

Since the mixture fit is terrible, let's try just the LLO model.

```{r}
# fit the model
m.bet.llo <- brm(
   bf(lo_bet ~ (1 + lo_ground_truth|worker_id) + 0 + condition + lo_ground_truth:condition # llo model
  ),
  data = model_df_llo,
  inits = 1, chains = 2, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=15),
  file = "stan/m_bet_llo"
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.bet.llo)
```

- Pairs plot

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# hyperparameters
pairs(m.bet.llo, pars = c("b_Intercept",
                          "sd_worker_id__Intercept",
                          "sd_worker_id__lo_ground_truth",
                          "cor_worker_id__Intercept__lo_ground_truth",
                          "sigma"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# slope and intercept effects
pairs(m.bet.llo, pars = c("b_conditionHOPs",
                          "b_conditionmeans_only",
                          "b_conditionintervals_w_means",
                          "b_lo_ground_truth:conditionHOPs",
                          "b_lo_ground_truth:conditionmeans_only",
                          "b_lo_ground_truth:conditionintervals_w_means"))
```

- Summary

```{r}
# model summary
print(m.bet.llo)
```

Let's check out a posterior predictive distribution for bets.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition,sd_diff,worker_id) %>%
  add_predicted_draws(m.bet.llo, prediction = "lo_bet", seed = 1234) %>%
  mutate(post_bet = plogis(lo_bet)) %>%
  ggplot(aes(x=post_bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for bets") +
  theme(panel.grid = element_blank())
```

How does this compare to the actual distribution of bet amounts?

```{r}
# posterior predictive check
model_df_llo %>%
  ggplot(aes(x=bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Empirical distribution of bets") +
  theme(panel.grid = element_blank())
```

What do the posterior for the slope of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.bet.llo) %>%
  transmute(slope_HOPs = `b_conditionHOPs:lo_ground_truth`,
            slope_intervals_w_means = `b_conditionintervals_w_means:lo_ground_truth`,
            slope_means_only = `b_conditionmeans_only:lo_ground_truth`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

What about the effects on intercepts?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.bet.llo) %>%
  transmute(intercept_HOPs = b_conditionHOPs,
            intercept_intervals_w_means = b_conditionintervals_w_means,
            intercept_means_only = b_conditionmeans_only) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for intercepts by visualization condition") +
  theme(panel.grid = element_blank())
```

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# this time we'll adopt functions from the tidybayes package to make plotting posterior predictions easier
model_df_llo %>%
  group_by(condition,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.bet.llo) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_bet, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo, alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_bet, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
```

What does this look like in probability units?

```{r}
model_df_llo %>%
  group_by(condition,worker_id) %>%
  data_grid(lo_ground_truth = seq_range(lo_ground_truth, n = 101)) %>%
  add_predicted_draws(m.bet.llo) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_bet), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo, alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(plogis(model_df_llo$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df_llo$lo_bet), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
```

This model looks alright, but there's a lot of variability which is not accounted for by the model.


##Adding Predictors to Deal with the Effect of Bet and Outcome on Previous Trial

Next, we'll try to account for some of the apparent noise in responses by modeling ways that responses might depend on the previous trial. Based on Prospect Theory, we should expect that betting behavior will depend on the payoff on the previous trial in terms of both the betting behavior and the outcome which produced that payoff. Lower pay on the previous trial resulting from a win on a small bet would result in a higher bet on the next trail. Conversely, lower pay on the previous trial resulting from a loss on a large bet would result in a lower bet on the next trial. However, it is ambiguous whether higher pay on the last trial resulting from a win on a large bet would result in a lower or higher bet on the next trail. In any case, it's worth investigating whether including information about the previous bet can improve our model.

###Add Columns to Model Dataframe for Previous Bet and Outcome

```{r}
# add columns for lo_bet and outcome on previous trial
model_df_llo <- model_df_llo %>%
  group_by(worker_id) %>%
  mutate(
    prev_lo_bet = lag(lo_bet, order_by = trial),
    prev_outcome = lag(as.logical(outcome), order_by = trial)
  ) %>%
  replace_na(list(prev_lo_bet = 0, prev_outcome= FALSE)) # consider setting this using practice trials
# model_df_llo %>%
#   select(worker_id, trial, lo_bet, prev_lo_bet, outcome, prev_outcome) %>%
#   arrange(worker_id,trial)
```

###Fit the Model

In this model, we separate parameters for worker effects, visualization effects, and effects of the previous trial into separate submodels in order to get more interpretable intercepts. Otherwise, all we've done is add effects for the previous trail on top of the parameters in the last model.

```{r}
# fit the model
m.bet.llo_prev <- brm(
   bf(lo_bet ~ worker + vis + trial,
      worker ~ (1 + lo_ground_truth|worker_id),
      vis~ 0 + condition + lo_ground_truth:condition,
      trial ~ prev_lo_bet*prev_outcome,
      nl = TRUE
  ),
  data = model_df_llo,
  prior = c(
    prior(normal(0, 1), class = b, nlpar = worker),
    prior(normal(0, 1), class = b, nlpar = vis),
    prior(normal(0, 1), class = b, nlpar = trial)
  ),
  inits = 1, chains = 2, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=15),
  file = "stan/m_bet_llo_prev"
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.bet.llo_prev)
```

- Pairs plot

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# hyperparameters
pairs(m.bet.llo_prev, pars = c("b_worker_Intercept",
                               "sd_worker_id__worker_Intercept",
                               "sd_worker_id__worker_lo_ground_truth",
                               "cor_worker_id__worker_Intercept__worker_lo_ground_truth",
                               "sigma"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# slope and intercept effects
pairs(m.bet.llo_prev, pars = c("b_vis_conditionHOPs",
                               "b_vis_conditionmeans_only",
                               "b_vis_conditionintervals_w_means"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# previous trial outcomes
pairs(m.bet.llo_prev, pars = c("b_trial_Intercept",
                               "b_trial_prev_lo_bet",
                               "b_trial_prev_lo_bet:prev_outcomeTRUE",
                               "b_trial_prev_outcomeTRUE"))
```

- Summary

```{r}
# model summary
print(m.bet.llo_prev)
```

Let's check out a posterior predictive distribution for bets.

```{r}
# posterior predictive check
model_df_llo %>%
  select(lo_ground_truth,condition,sd_diff,worker_id,prev_lo_bet,prev_outcome) %>%
  add_predicted_draws(m.bet.llo_prev, prediction = "lo_bet", seed = 1234) %>%
  mutate(post_bet = plogis(lo_bet)) %>%
  ggplot(aes(x=post_bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for bets") +
  theme(panel.grid = element_blank())
```

How does this compare to the actual distribution of bet amounts?

```{r}
# posterior predictive check
model_df_llo %>%
  ggplot(aes(x=bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Empirical distribution of bets") +
  theme(panel.grid = element_blank())
```

What do the posterior for the slope of each visualization condition look like?

```{r}
# use posterior samples to define distributions for the slope in each visualization condition
posterior_samples(m.bet.llo_prev) %>%
  transmute(slope_HOPs = `b_vis_conditionHOPs:lo_ground_truth`,
            slope_intervals_w_means = `b_vis_conditionintervals_w_means:lo_ground_truth`,
            slope_means_only = `b_vis_conditionmeans_only:lo_ground_truth`) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

What about the effects on intercepts?

```{r}
# use posterior samples to define distributions for the intercept in each visualization condition
posterior_samples(m.bet.llo_prev) %>%
  transmute(intercept_HOPs = b_vis_conditionHOPs,
            intercept_intervals_w_means = b_vis_conditionintervals_w_means,
            intercept_means_only = b_vis_conditionmeans_only) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for intercepts by visualization condition") +
  theme(panel.grid = element_blank())
```

Let's take a look at some of the estimated linear models per visualization condition.

```{r}
# need to extend memory limit in .Renviron to run this (https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached)
model_df_llo %>%
  group_by(condition,worker_id) %>%
  data_grid(
    lo_ground_truth = seq_range(lo_ground_truth, n = 21),
    prev_lo_bet = seq_range(prev_lo_bet, n = 21),
    prev_outcome = c(TRUE, FALSE)) %>%
  add_predicted_draws(m.bet.llo_prev, n = 500) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_bet, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo, alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(model_df_llo$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df_llo$lo_bet, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(. ~ condition)
```

What does this look like in probability units?

```{r}
model_df_llo %>%
  group_by(condition,sd_diff,worker_id) %>%
  data_grid(
    lo_ground_truth = seq_range(lo_ground_truth, n = 21),
    prev_lo_bet = seq_range(prev_lo_bet, n = 21),
    prev_outcome = c(TRUE, FALSE)) %>%
  add_predicted_draws(m.bet.llo_prev, n = 500) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_bet), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = 1/4) +
  geom_point(data = model_df_llo, alpha = 0.3) +
  scale_fill_brewer(type = "qual", palette = 1) +
  scale_color_brewer(type = "qual", palette = 1) + 
  coord_cartesian(xlim = quantile(plogis(model_df_llo$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df_llo$lo_bet), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_grid(sd_diff ~ condition)
```

Let's also look at the posteriors for the effects of the previous trial.

```{r}
# use posterior samples to define distributions for the intercept in each visualization condition
posterior_samples(m.bet.llo_prev) %>%
  transmute(slope_prev_bet_lose = b_trial_prev_lo_bet,
            slope_prev_bet_win = b_trial_prev_lo_bet + `b_trial_prev_lo_bet:prev_outcomeTRUE`,
            intercept_lose = b_trial_Intercept,
            intercept_win = b_trial_prev_outcomeTRUE
            ) %>%
  gather(key, value) %>%
  ggplot(aes(x = value, group = key, color = key, fill = key)) +
  geom_density(alpha = 0.3) +
  # scale_fill_brewer(type = "qual", palette = 1) +
  # scale_color_brewer(type = "qual", palette = 1) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for intercepts by visualization condition") +
  theme(panel.grid = element_blank())
```

The problem with this model is that the effect of the previous trial is small and does not change the difference between the posterior predictive distribution of bets under the LLO model and the empirical distribution of bets. In particular, the model does not caputure the modal bet near 500 (the middle of the scale).

Did we really get anything out of adding these previous trial predictors to the model? Let's compare the two versions of the LLO model that we've fit for bets based on widely applicable information criteria.

```{r}
model_weights(m.bet.llo, m.bet.llo_prev, weights = "waic")
```

It looks like the model without the predictors for the previous trial performed better.


##Ordinal Model of Betting

Since the distribution of bets is trimodal, and we cannot seem to account for the center mode in the LLO model, let's treat user responses as reflecting a choice between three strategies: bet low, bet middle, and bet high.

###Categorizing Bets

We'll split bets into three categories: low, middle, and high. We'll use split points at 334 and 667 on a scale from 1 to 1000.

```{r}
# add column for ordinal_bet
model_df_ordinal <- model_df_llo %>%
  mutate(
    ordinal_bet = if_else(bet < 334, 1, if_else(bet < 667, 2, 3)) # {1: low, 2: mid, 3, high}
  )
# model_df_ordinal %>%
#   select(worker_id, trial, bet, ordinal_bet) %>%
#   arrange(worker_id,trial)
```

###Fit the Model

This model is has the same structure as a our LLO model, but it models bets as log odds of ordinal responses rather than a log odds transform of the proportion of the budget allocated to the bet.

```{r}
# fit the model
m.bet.ordinal <- brm(
  bf(ordinal_bet ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth:condition),
  data = model_df_ordinal,
  family=cumulative("logit"),
  inits = 1, chains = 1, cores = 2,
  control = list(adapt_delta = 0.999, max_treedepth=15),
  file = "stan/m_bet_ordinal"
)
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.bet.ordinal)
```

- Pairs plot

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# hyperparameters
pairs(m.bet.ordinal, pars = c("b_worker_Intercept",
                               "sd_worker_id__worker_Intercept",
                               "sd_worker_id__worker_lo_ground_truth",
                               "cor_worker_id__worker_Intercept__worker_lo_ground_truth",
                               "sigma"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# slope and intercept effects
pairs(m.bet.ordinal, pars = c("b_vis_conditionHOPs",
                               "b_vis_conditionmeans_only",
                               "b_vis_conditionintervals_w_means"))
```

```{r}
# pairs plot (too many things to view at once, so we've grouped them)
# previous trial outcomes
pairs(m.bet.ordinal, pars = c("b_trial_Intercept",
                               "b_trial_prev_lo_bet",
                               "b_trial_prev_lo_bet:prev_outcomeTRUE",
                               "b_trial_prev_outcomeTRUE"))
```

- Summary

```{r}
# model summary
print(m.bet.ordinal)
```

Let's check out a posterior predictive distribution for bets.

```{r}
# posterior predictive check
model_df_ordinal %>%
  select(lo_ground_truth,condition,sd_diff,worker_id) %>%
  add_predicted_draws(m.bet.llo_prev, prediction = "ordinal_bet", seed = 1234) %>%
  mutate(post_bet = ordinal_bet) %>%
  ggplot(aes(x=post_bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for bets") +
  theme(panel.grid = element_blank())
```

How does this compare to the actual distribution of bet amounts?

```{r}
# posterior predictive check
model_df_ordinal %>%
  ggplot(aes(x=ordinal_bet)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Empirical distribution of bets") +
  theme(panel.grid = element_blank())
```


<!-- ##Revised Mixture for Betting -->

<!-- Let's try reintroducing a mixture component to the better fitting LLO model from above. This time we will not allow the constant response to vary by subject. Instead, it will be centered on the middle of the betting scale. People might bet in the middle when they aren't sure what to do and want to avoid the guaranteed loss of the flat tax. -->

<!-- ```{r} -->
<!-- # define stanvars for multi_normal prior on condition effects -->
<!-- stanvars <- stanvar(rep(1, 3), "mu_theta2", scode = "  vector[3] mu_theta2;") + -->
<!--   stanvar(diag(3), "Sigma_theta2", scode = "  matrix[3, 3] Sigma_theta2;") -->
<!-- # fit the model -->
<!-- m.bet.llo_mix <- brm( -->
<!--    bf(lo_bet ~ 1,  -->
<!--     mu1 ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth:condition, # llo model -->
<!--     mu2 ~ 1, # constant response of 500 to account for mode near the middle of the betting scale -->
<!--     theta2 ~ (1|worker_id) + 0 + condition # the proportion of responses that are constant in each condition -->
<!--   ), -->
<!--   data = model_df_llo, -->
<!--   family = mixture(gaussian, gaussian, order = 'mu'), -->
<!--   prior = c( -->
<!--     prior(normal(0, 1), class = Intercept, dpar = mu1), -->
<!--     prior(normal(0, 1), class = Intercept, dpar = mu2), -->
<!--     prior("multi_normal(mu_theta2, Sigma_theta2)", class = b, dpar = theta2) -->
<!--   ), -->
<!--   stanvars = stanvars, -->
<!--   inits = 1, chains = 2, cores = 2, -->
<!--   control = list(adapt_delta = 0.999, max_treedepth=15), -->
<!--   file = "stan/m_bet_llo_mix2" -->
<!-- ) -->
<!-- ``` -->