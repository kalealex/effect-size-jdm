---
title: "Building a Linear Log Odds Model of Probability of Superiority"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(RColorBrewer)
library(rstan)
library(bayesplot)
library(modelr)
library(tidybayes)
library(ggstance)
library(brms)
library(loo)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
devAskNewPage(ask = FALSE)
```

In this document, we build a linear log odds model of probability of superiority judgments through a process of model expansion, where we will gradually add predictors to our model.

The LLO model follows from [related work](https://www.frontiersin.org/articles/10.3389/fnins.2012.00001/full) suggesting that the human perception of probability is encoded on a log odds scale. On this scale, the slope of a linear model represents the shape and severity of the function describing bias in probability perception. The greater the deviation of from a slope of 1 (i.e., ideal performance), the more biased the judgments of probability. Slopes less than one correspond to the kind of bias predicted by excessive attention to the mean. On the same log odds scale, the intercept is a crossover-point which should be proportional to the number of categories of possible outcomes among which probability is divided. In our case, the intercept should be about 0.5 since workers are judging the probability of a team getting more points with a new player than without.

##Load and Prepare Data

We load worker responses from our experiment and do some preprocessing.

```{r}
# read in data 
full_df <- read_csv("experiment-anonymous.csv")

# preprocessing
responses_df <- full_df %>%
  rename( # rename to convert away from camel case
    worker_id = workerId,
    ground_truth = groundTruth,
    sd_diff = sdDiff,
    p_award_with = pAwardWith,
    p_award_without = pAwardWithout,
    account_value = accountValue,
    p_superiority = pSup,
    start_time = startTime,
    resp_time = respTime,
    trial_dur = trialDur,
    trial_idx = trialIdx
  ) %>%
  # remove practice and mock trials from responses dataframe, leave in full version
  filter(trial_idx != "practice", trial_idx != "mock") %>% 
  # drop rows where p_superiority == NA for some reason
  drop_na(p_superiority) %>%
  # mutate rows where intervene == -1 for some reason
  mutate(
    intervene = if_else(intervene == -1,
                        # repair
                        if_else((payoff == (award_value - 1) | payoff == -1),
                                1, # payed for intervention
                                0), # didn't pay for intervention
                        # don't repair
                        as.numeric(intervene) # hack to avoid type error
                        )
  ) %>%
  # set up factors for modeling
  mutate(
    # add a variable to note whether the chart they viewed showed means
    means = as.factor((start_means & as.numeric(trial) <= (n_trials / 2)) | (!start_means & as.numeric(trial) > (n_trials / 2))),
    start_means = as.factor(start_means),
    sd_diff = as.factor(sd_diff),
    trial_number = as.numeric(trial)
  )

head(responses_df)
```

We need the data in a format where it is prepared for modeling. We censor responses to the range 0.5% to 99.5% where responses at these bounds reflect an intended response at the bound or higher. By rounding responses to the nearest 0.5%, we assume that the response scale has a resolution of 1% in practice. We need to do this to avoid values of positive or negative infinity when we transform responses to a log odds scale. We convert both probability of superiority judgments and the ground truth to a logit scale. 

```{r}
# create data frame for model
model_df <- responses_df %>%
  mutate( 
    # recode responses greater than 99.5% and less than 0.5% to avoid values of +/- Inf on a logit scale
    p_superiority = if_else(p_superiority > 99.5, 
                            99.5,
                            if_else(p_superiority < 0.5,
                                    0.5,
                                    as.numeric(p_superiority))),
    # apply logit function to p_sup judgments and ground truth
    lo_p_sup = qlogis(p_superiority / 100),
    lo_ground_truth = qlogis(ground_truth),
    # # scale and center lo_ground_truth
    # clo_ground_truth = (lo_ground_truth - mean(lo_ground_truth)) / (max(lo_ground_truth) - min(lo_ground_truth)),
    # scale and center trial order
    trial = (trial_number - as.numeric(n_trials) / 2) / as.numeric(n_trials)
  )
```

Now, lets apply our exclusion criteria, cutting our sample down to only the subset of participants who passed both attention checks.

```{r}
# determine exclusions
exclude_df <- model_df %>% 
  # attention check trials where ground truth = c(0.5, 0.999)
  mutate(failed_check = (ground_truth == 0.5 & intervene != 0) | (ground_truth == 0.999 & intervene != 1)) %>%
  group_by(worker_id) %>%
  summarise(
    failed_attention_checks = sum(failed_check),
    unique_p_sup = length(unique(p_superiority)),
    # excluded if they failed either attention check or used fewer than three levels of the response scale
    exclude = failed_attention_checks > 0 | unique_p_sup < 3
  ) %>% 
  dplyr::select(worker_id, exclude)

# apply exclusion criteria and remove attention check trials from modeling data set
model_df <- model_df %>% 
  left_join(exclude_df, by = "worker_id") %>% 
  filter(exclude == FALSE) %>%
  filter(ground_truth > 0.5 & ground_truth < 0.999)

# how many remaining workers per condition?
model_df %>%
  group_by(condition, start_means) %>% # between subject manipulations
  summarise(
    n_workers = length(unique(worker_id))
  )
```

In addition to excluding participants who failed at least one of the two attention checks in the experiment, which is our preregistered exclusion criterion, we also exclude a handful of workers whose data lead to model fit issues. These are workers who responded with only one or two levels of the probability of superiority scale. We could make the case that these workers might not have been trying very hard when responding, but the reason for excluding them is much more practical: It is not possible for the modeling process we are using to estimate random effects on response variability for these participants (i.e., you cannot calculate the variance of a set with only one or two distinct values). These random effects on variance are very important, because our data almost certaintly violate a homogeneity of variance assumption. 

Because of these exclusions, we are a few participants short of our target samples size of 80. We should still have more than enough data to support statistical inferences. Here we drop a handful of additional participants to maintain counterbalancing of block order. Since we know that there were some participants with dropped responses, let's prioritize leaving out workers with the greatest number of dropped trials in each counterbalancing condition.

```{r}
model_df %>%
  group_by(condition, start_means, worker_id) %>%
  summarise(
    n_trials = n(),
    dropped_trials = 32 - n_trials
  ) %>%
  filter(dropped_trials > 0)
```

Based on a comparison of the two tables above, we'll drop workers c488db75, ce016e09, and f430e2e8 to ensure our ability to fit our model.

```{r}
# remove workers with missing data, plus one where condition = densities, start_means = FALSE
model_df <- model_df %>%
filter(!worker_id %in% c("c488db75", "ce016e09", "f430e2e8")) # also exclude "c337674a" to counterbalance, but would take a long time to rerun

model_df %>%
  group_by(condition, start_means) %>% # between subject manipulations
  summarise(
    n_workers = length(unique(worker_id))
  )
```

Now we have our dataset ready for modeling.


##Distribution of Probability of Superiority Judgments

We start as simply as possible by just modeling the distribution of probability of superiority judgements on the log odds scale.

Before we fit the model to our data, let's check that our priors seem reasonable. We'll use a weakly informative prior for the intercept parameter since we want the population-level centered intercept to be flexible. We set the expected value of the prior on the intercept equal to the mean value of the ground truth that we sampled (in log odds units).

```{r}
# get mean value of ground truth sampled in log odds units
model_df %>% select(lo_ground_truth) %>% summarize(mean = mean(lo_ground_truth))
```


```{r}
# get_prior(data = model_df, family = "gaussian", formula = lo_p_sup ~ 1)

# starting as simple as possible: learn the distribution of lo_p_sup
prior.lo_p_sup <- brm(data = model_df, family = "gaussian",
              lo_p_sup ~ 1,
              prior = c(prior(normal(1.3, 1), class = Intercept),
                        prior(normal(0, 1), class = sigma)),
              sample_prior = "only",
              iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Let's look at our prior predictive distribution. For this intercept model, it should be skewwed left because we have located our prior near 74% probability of superiority. We should see a peak near the upper bound of the probability scale.

```{r}
# prior predictive check
model_df %>%
  select() %>%
  add_predicted_draws(prior.lo_p_sup, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    prior_p_sup = plogis(lo_p_sup)
  ) %>%
  ggplot(aes(x = prior_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Prior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Now, let's fit the model to data. This is just trying to estimate the mean response regardless of the ground truth.

```{r}
# starting as simple as possible: learn the distribution of lo_p_sup
m.lo_p_sup <- brm(data = model_df, family = "gaussian",
              lo_p_sup ~ 1,
              prior = c(prior(normal(1.3, 1), class = Intercept),
                        prior(normal(0, 1), class = sigma)),
              iter = 3000, warmup = 500, chains = 2, cores = 2,
              file = "model-fits/lo_mdl")
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.lo_p_sup)
```

- Pairs plot

```{r}
# pairs plot
pairs(m.lo_p_sup)
```

- Summary

```{r}
# model summary
print(m.lo_p_sup)
```

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select() %>%
  add_predicted_draws(m.lo_p_sup, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
    ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority",
       post_p_sup = NULL) +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Our model is not sensitive to the ground truth, so we expect to see a mismatch here.


##Linear Log Odds Model of Probability of Superiority

Now well add in a slope parameter to make our model sensitive to the ground truth. This is the simplest version of our linear log odds (LLO) model.

Before we fit the model to our data, let's check that our priors seem reasonable. Since we are now including a slope parameter for the ground truth in our model, we can dial down the width of our prior for sigma (i.e., residual variance) to avoid over-dispersion of predicted responses.

```{r}
# get_prior(data = model_df, family = "gaussian", formula = lo_p_sup ~ lo_ground_truth)

# simple LLO model
prior.llo <- brm(data = model_df, family = "gaussian",
                 lo_p_sup ~ lo_ground_truth,
                 prior = c(prior(normal(1, 0.5), class = b),
                           prior(normal(1.3, 1), class = Intercept),
                           prior(normal(0, 0.5), class = sigma)),
                 sample_prior = "only",
                 iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Let's look at our prior predictive distribution. For this linear model, we should see density spread slightly more evenly across probability values. 

```{r}
# prior predictive check
model_df %>%
  select(lo_ground_truth) %>%
  add_predicted_draws(prior.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    prior_p_sup = plogis(lo_p_sup)
    ) %>%
  ggplot(aes(x = prior_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Prior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Now let's fit the model to data.

```{r}
# simple LLO model
m.llo <- brm(data = model_df, family = "gaussian",
             lo_p_sup ~ lo_ground_truth,
             prior = c(prior(normal(1, 0.5), class = b),
                       prior(normal(1.3, 1), class = Intercept),
                       prior(normal(0, 0.5), class = sigma)),
             iter = 3000, warmup = 500, chains = 2, cores = 2,
             file = "model-fits/llo_mdl")
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.llo)
```

- Pairs plot

```{r}
# pairs plot
pairs(m.llo)
```

Our slope and intercept parameters seem pretty highly correlated. Maybe adding hierarchy to our model will remedy this.

- Summary

```{r}
# model summary
print(m.llo)
```

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select(lo_ground_truth) %>%
  add_predicted_draws(m.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
  ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Our model is now sensitive to the ground truth, but it is still having trouble fitting the data. It may be that the model is not capturing the individual variability in response patterns. Next we'll add hierarchy to our model.


##Add Hierarchy for Slope, Intercepts, and Sigma

The models we've created thus far fail to account for much of the variability in the data. Here, we attempt to parse some heterogeniety in responses by modeling a random effect of worker on slopes, intercepts, and residual variance. This introduces a hierarchical component to our model in order to account for individual differences in the best fitting linear model for each worker's data.

Before we fit the model to our data, let's check that our priors seem reasonable. We are adding hyperpriors for the standard deviation of slopes, intercepts, and residual variation (i.e., sigma) per worker, as well as the correlation between them. We'll set moderately wide priors on these worker-level slope and intercept effects. We want some regularization, but we don't want to overregularize potentially large individual variability, which is sort of a tough balance. We'll also narrow the priors on sigma parameters since we are now attributing variability to more sources and we want to avoid overdispersion. We'll set a prior on the correlation between slopes and intercepts per worker that avoids large absolute correlations.

```{r}
# get_prior(data = model_df, family = "gaussian", formula = bf(lo_p_sup ~ (1 + lo_ground_truth|sharecor|worker_id) + lo_ground_truth, sigma ~ (1|sharecor|worker_id)))

# hierarchical LLO model
prior.wrkr.llo <- brm(data = model_df, family = "gaussian",
                      formula = bf(lo_p_sup ~ (1 + lo_ground_truth|sharecor|worker_id) + lo_ground_truth, 
                                   sigma ~ (1|sharecor|worker_id)),
                      prior = c(prior(normal(1, 0.5), class = b),
                                prior(normal(1.3, 1), class = Intercept),
                                prior(normal(0, 0.15), class = sd, group = worker_id),
                                prior(normal(0, 0.15), class = sd, dpar = sigma),
                                prior(lkj(4), class = cor)),
                      sample_prior = "only",
                      iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Let's look at our prior predictive distribution. Because this model contains so many more sources of variation, the prior predictive distribution may look a little overdispersed (i.e., lots of mass at the boundaries of the response scale). However, it's probably best to err on the side of not making our priors on individual parameters too narrow.

```{r}
# prior predictive check
model_df %>%
  select(lo_ground_truth, worker_id) %>%
  add_predicted_draws(prior.wrkr.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    prior_p_sup = plogis(lo_p_sup)
    ) %>%
  ggplot(aes(x = prior_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Prior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Now, let's fit the model to our data.

```{r}
# hierarchical LLO model
m.wrkr.llo <- brm(data = model_df, family = "gaussian",
                  formula = bf(lo_p_sup ~ (1 + lo_ground_truth|sharecor|worker_id) + lo_ground_truth,  
                               sigma ~ (1|sharecor|worker_id)), 
                  prior = c(prior(normal(1, 0.5), class = b),
                            prior(normal(1.3, 1), class = Intercept),
                            prior(normal(0, 0.15), class = sd, group = worker_id),
                            prior(normal(0, 0.15), class = sd, dpar = sigma),
                            prior(lkj(4), class = cor)),
                  iter = 3000, warmup = 500, chains = 2, cores = 2,
                  control = list(adapt_delta = 0.99, max_treedepth = 12),
                  file = "model-fits/llo_mdl-wrkr")
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.wrkr.llo)
```

- Pairs plot

```{r}
# pairs plot (fixed effects)
pairs(m.wrkr.llo, exact_match = TRUE, pars = c("b_Intercept", "b_lo_ground_truth", "b_sigma_Intercept"))
```

```{r}
# pairs plot (random effects)
pairs(m.wrkr.llo, pars = c("sd_worker_id__", "cor_worker_id__"))
```

- Summary

```{r}
# model summary
print(m.wrkr.llo)
```

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select(lo_ground_truth, worker_id) %>%
  add_predicted_draws(m.wrkr.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
    ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```
 
Running a leave one out posterior predictive check, we can see that overall this model has decent predictive validity.
 
```{r}
# set up data for LOO posterior predictive check
y <- model_df$lo_p_sup
yrep <- posterior_predict(m.wrkr.llo)

# run LOO to get weights
loo <- loo(m.wrkr.llo, save_psis = TRUE, cores = 2)
psis <- loo$psis_object
lw <- weights(psis)
```

```{r}
ppc_loo_pit_qq(y, yrep, lw = lw)
```

Let's look at posterior predictions per worker to get a more detailed sense of fit quality. When we make this kind of plot for model checks at the level of individual workers, we'll look at a subset of workers to keep the number of charts generated to a reasonable number.

```{r}
# two workers from each counterbalancing condition
model_check_set <- model_df %>% 
  group_by(start_means, condition, worker_id) %>%
  summarise() %>%
  top_n(2)
model_check_set <- model_check_set$worker_id
model_check_df <- model_df %>%
  filter(worker_id %in% model_check_set)

model_check_df %>% 
  group_by(worker_id) %>%
  summarise()
```
  
```{r}
model_check_df %>%
  # get posterior predictive distribution
  group_by(lo_ground_truth, worker_id) %>%
  add_predicted_draws(m.wrkr.llo, n = 500) %>%
  # plot
  ggplot(aes(x = lo_ground_truth, y = lo_p_sup, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(model_df$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df$lo_p_sup, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

What does this look like in probability units?

```{r}
model_check_df %>%
  # get posterior predictive distribution
  group_by(lo_ground_truth, worker_id) %>%
  add_predicted_draws(m.wrkr.llo, n = 500) %>%
  # plot
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

One thing we're trying to gage here is whether our model has predictive validity at the level of each worker. To examine this more closely we'll look at QQ plots for residuals at the worker level. 

```{r}
model_check_df %>%
  # get posterior draws and transform
  add_predicted_draws(m.wrkr.llo, n = 500) %>%
  group_by(lo_ground_truth, worker_id) %>%
  summarise(
    p_residual = mean(.prediction < lo_p_sup), # what proportion of predicted judgments are less than the observed response?
    z_residual = qnorm(p_residual)             # what are the z-scores of these cumulative probabilities?
  ) %>%
  # plot
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline() +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

These don't look great. We can see that there is some clustering of responses, probably reflecting a preference for round numbers on the response scale.

```{r}
pp_check(m.wrkr.llo)
```

As long as the location and scale of the predictions look reasonably in line with the empirical data (which they do), we don't really care too much if the model doesn't predict every small anomally. This plot showing predictive densities alongside the observed data is resassuring insofar as we are doing a decent job of modeling the things we care about.

Let's see if our predictive validity improves at the worker level when we add our experimental manipulations as predictors.


##Add Predictors to Answer Research Questions

In order to answer our research questions, we need to account for the interaction of the ground truth with whether means are present vs absent, whether visualized uncertainty is high vs low, and what uncertainty visualization condition a user was assigned to. We'll add predictors for each of these factors to our hierarchical model in turn.

###Presence/Absence of the Mean

Our primary research question is how the presence of the mean impacts the slopes of linear models in log odds space. To test this, we'll add an interaction between the presence of the mean and the ground truth.

We use the same priors as we did for the previous model. Now, let's fit the model to our data.

```{r}
# hierarchical LLO model with fixed effects on slope and residual variance conditioned on the presence/absence of the mean
m.wrkr.means.llo <- brm(data = model_df, family = "gaussian",
                        formula = bf(lo_p_sup ~  (1 + lo_ground_truth|sharecor|worker_id) + lo_ground_truth*means,
                                     sigma ~ (1|sharecor|worker_id)),
                        prior = c(prior(normal(1, 0.5), class = b),
                                  prior(normal(1.3, 1), class = Intercept),
                                  prior(normal(0, 0.15), class = sd, group = worker_id),
                                  prior(normal(0, 0.15), class = sd, dpar = sigma),
                                  prior(lkj(4), class = cor)),
                        iter = 3000, warmup = 500, chains = 2, cores = 2,
                        control = list(adapt_delta = 0.99, max_treedepth = 12),
                        file = "model-fits/llo_mdl-wrkr_means")
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.wrkr.means.llo)
```

- Pairs plot

```{r}
# pairs plot (fixed effects)
pairs(m.wrkr.means.llo, exact_match = TRUE, pars = c("b_Intercept", 
                                                           "b_lo_ground_truth",
                                                           "b_meansTRUE",
                                                           "b_lo_ground_truth:meansTRUE",
                                                           "b_sigma_Intercept"))
```

```{r}
# pairs plot (random effects)
pairs(m.wrkr.means.llo, exact_match = TRUE, pars = c("sd_worker_id__Intercept", 
                                                     "sd_worker_id__lo_ground_truth",
                                                     "sd_worker_id__sigma_Intercept"))
```

```{r}
# pairs plot (covariance matrix)
pairs(m.wrkr.means.llo, pars = c("cor_worker_id__"))
```

- Summary

```{r}
# model summary
print(m.wrkr.means.llo)
```

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select(lo_ground_truth, worker_id, means) %>%
  add_predicted_draws(m.wrkr.means.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
  ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Running a leave one out posterior predictive check, we can see that overall this model has decent predictive validity.
 
```{r}
# set up data for LOO posterior predictive check
y <- model_df$lo_p_sup
yrep <- posterior_predict(m.wrkr.means.llo)

# run LOO to get weights
loo <- loo(m.wrkr.means.llo, save_psis = TRUE, cores = 2)
psis <- loo$psis_object
lw <- weights(psis)
```

```{r}
ppc_loo_pit_qq(y, yrep, lw = lw)
```

Let's take a look at predictions per worker and visualization condition to get a more granular sense of our model fit.

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means) %>%
  add_predicted_draws(m.wrkr.means.llo, n = 500) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_p_sup, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(model_df$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df$lo_p_sup, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

What does this look like in probability units?

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means) %>%
  add_predicted_draws(m.wrkr.means.llo, n = 500) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

To examine more closely whether our model has predictive validity at the level of each worker, we'll look at QQ plots for residuals at the worker level.

```{r}
model_check_df %>%
  add_predicted_draws(m.wrkr.means.llo, n = 500) %>%
  group_by(lo_ground_truth, worker_id) %>%
  summarise(
    p_residual = mean(.prediction < lo_p_sup), # what proportion of predicted judgments are less than the observed response?
    z_residual = qnorm(p_residual)             # what are the z-scores of these cumulative probabilities?
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline() +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

These still look pretty terrible.

With this model we can take a first stab at addressing our research question about the presence of extrinsic means. What does the posterior for the slope of the LLO model look like when means are present vs absent, ignoring other manipulations for now? Since we are building a complex model, we'll forego calculating maringal effects by manually combining parameters. Instead we'll use add_fitted_draws and compare_levels from tidybayes to get our slopes, and then we'll take their weighted average grouping by the parameters for which we want marginal effects.

```{r}
model_df %>%
  group_by(means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.wrkr.means.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, .draw) %>%                        # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out visualization condition by taking a weighted average
  ggplot(aes(x = slope, group = means, color = means, fill = means)) +
  geom_density(alpha = 0.35) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes for mean present/absent") +
  theme(panel.grid = element_blank())
```

Recall that a slope of 1 represents no bias. This chart suggests that people are biased with or without adding means. We should not be surprised to see little to no effect in this model. The mean difference is a good heuristic for probability of superiority when variance of visualized estimates is high, but it is not a good heuristic when variance is low. Thus, we should expect to see the effect we are looking for as an interaction between the presence of the mean and the level of uncertainty.

###Level of Uncertainty Shown

Another factor that we manipulate is the level of uncertainty presented to chart users. We expect level of uncertainty (sd_diff) to determine the impact of extrinsic means on performance. To test this, we'll add an interaction between sd_diff, means, and the ground truth.

We use the same priors as we did for the previous model. Now, let's fit the model to our data.

```{r}
# hierarchical LLO model
m.wrkr.means.sd.llo <- brm(data = model_df, family = "gaussian",
                           formula = bf(lo_p_sup ~  (1 + lo_ground_truth|sharecor|worker_id) + lo_ground_truth*means*sd_diff,
                                        sigma ~ (1|sharecor|worker_id)),
                           prior = c(prior(normal(1, 0.5), class = b),
                                     prior(normal(1.3, 1), class = Intercept),
                                     prior(normal(0, 0.15), class = sd, group = worker_id),
                                     # prior(normal(0, 0.3), class = b, dpar = sigma),
                                     prior(normal(0, 0.15), class = sd, dpar = sigma),
                                     prior(lkj(4), class = cor)),
                           iter = 3000, warmup = 500, chains = 2, cores = 2,
                           control = list(adapt_delta = 0.99, max_treedepth = 12),
                           file = "model-fits/llo_mdl-wrkr_means_sd")
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.wrkr.means.sd.llo)
```

- Pairs plot

```{r}
# pairs plot (LLO params)
pairs(m.wrkr.means.sd.llo, exact_match = TRUE, pars = c("b_Intercept", 
                                                        "b_lo_ground_truth",
                                                        "b_meansTRUE",
                                                        "b_sd_diff15",
                                                        "b_lo_ground_truth:meansTRUE",
                                                        "b_lo_ground_truth:sd_diff15",
                                                        "b_meansTRUE:sd_diff15",
                                                        "b_lo_ground_truth:meansTRUE:sd_diff15"))
```

```{r}
# pairs plot (random effects on lo_p_sup)
pairs(m.wrkr.means.sd.llo, exact_match = TRUE, pars = c("sd_worker_id__Intercept", 
                                                        "sd_worker_id__lo_ground_truth"))
```

```{r}
# pairs plot (sigma params)
pairs(m.wrkr.means.sd.llo, exact_match = TRUE, pars = c("b_sigma_Intercept", 
                                                        "sd_worker_id__sigma_Intercept"))
```

```{r}
pairs(m.wrkr.means.sd.llo, pars = c("cor_worker_id__"))
```

- Summary

```{r}
# model summary
print(m.wrkr.means.sd.llo)
```

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select(lo_ground_truth, worker_id, means, sd_diff) %>%
  add_predicted_draws(m.wrkr.means.sd.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
  ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Running a leave one out posterior predictive check, we can see that overall this model has decent predictive validity.
 
```{r}
# set up data for LOO posterior predictive check
y <- model_df$lo_p_sup
yrep <- posterior_predict(m.wrkr.means.sd.llo)

# run LOO to get weights
loo <- loo(m.wrkr.means.sd.llo, save_psis = TRUE, cores = 2)
psis <- loo$psis_object
lw <- weights(psis)
```

```{r}
ppc_loo_pit_qq(y, yrep, lw = lw)
```

Let's take a look at predictions per worker and visualization condition to get a more granular sense of our model fit.

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means, sd_diff) %>%
  add_predicted_draws(m.wrkr.means.sd.llo, n = 500) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_p_sup, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(model_df$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df$lo_p_sup, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

What does this look like in probability units?

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means, sd_diff) %>%
  add_predicted_draws(m.wrkr.means.sd.llo, n = 500) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

To examine more closely whether our model has predictive validity at the level of each worker, we'll look at QQ plots for residuals at the worker level.

```{r}
model_check_df %>%
  add_predicted_draws(m.wrkr.llo, n = 500) %>%
  group_by(lo_ground_truth, worker_id) %>%
  summarise(
    p_residual = mean(.prediction < lo_p_sup), # what proportion of predicted judgments are less than the observed response?
    z_residual = qnorm(p_residual)             # what are the z-scores of these cumulative probabilities?
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline() +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

These still look pretty terrible.

What does the posterior for the slope of the LLO model look like when means are present vs absent at different levels of uncertainty, ignoring other manipulations?

```{r}
model_df %>%
  group_by(means, sd_diff) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.wrkr.means.sd.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, .draw) %>%               # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out visualization condition by taking a weighted average
  ggplot(aes(x = slope, group = means, color = means, fill = means)) +
  geom_density(alpha = 0.35) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes for mean present/absent") +
  theme(panel.grid = element_blank()) +
  facet_grid(. ~ sd_diff)
```

Recall that a slope of 1 represents no bias. Overall, people seem less biased at baseline when uncertainty is higher. With regard to the interaction, we see about what we expect. Adding means makes responses less biased when uncertainty is high. However, we also expected to see the opposite as well, that adding means would make people more biased when uncertainty is low. Maybe this will turn out only to be the case for some uncertainty visualization formats rather than across the board. 

###Visualization Condition

The other thing we really want to know about is the impact of visualization condition on the slopes of linear models in log odds space. Do some visualizations lead to more extreme patterns of bias than others? To test this, we'll add an interaction between visualization condition and the ground truth. Now we have all our predictors of interest in one model (i.e., this will be the minimal model required to answer our research questions).

We use the same priors as we did for the previous model. Now, let's fit the model to our data.

```{r}
# minimal LLO model
m.m.llo <- brm(data = model_df, family = "gaussian",
               formula = bf(lo_p_sup ~  (1 + lo_ground_truth|sharecor|worker_id) + lo_ground_truth*means*sd_diff*condition,
                            sigma ~ (1|sharecor|worker_id)),
               prior = c(prior(normal(1, 0.5), class = b),
                         prior(normal(1.3, 1), class = Intercept),
                         prior(normal(0, 0.15), class = sd, group = worker_id),
                         # prior(normal(0, 0.3), class = b, dpar = sigma),
                         prior(normal(0, 0.15), class = sd, dpar = sigma),
                         prior(lkj(4), class = cor)),
               iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
               control = list(adapt_delta = 0.99, max_treedepth = 12),
               file = "model-fits/llo_mdl-minimal")
```

Check diagnostics:

- Trace plots

```{r}
# trace plots
plot(m.m.llo)
```

- Pairs plot

```{r}
# pairs plot (intercepts)
pairs(m.m.llo, exact_match = TRUE, pars = c("b_Intercept",
                                            "b_lo_ground_truth",
                                            "b_meansTRUE",
                                            "b_sd_diff15",
                                            "b_conditionintervals",
                                            "b_meansTRUE:sd_diff15",
                                            "b_meansTRUE:conditionintervals",
                                            "b_sd_diff15:conditionintervals",
                                            "b_meansTRUE:sd_diff15:conditionintervals"))
```

```{r}
# pairs plot (LLO slopes)
pairs(m.m.llo, exact_match = TRUE, pars = c("b_lo_ground_truth:meansTRUE",
                                            "b_lo_ground_truth:sd_diff15",
                                            "b_lo_ground_truth:conditionintervals",
                                            "b_lo_ground_truth:meansTRUE:sd_diff15",
                                            "b_lo_ground_truth:meansTRUE:conditionintervals",
                                            "b_lo_ground_truth:sd_diff15:conditionintervals",
                                            "b_lo_ground_truth:meansTRUE:sd_diff15:conditionintervals"))
```

```{r}
# pairs plot (random effects)
pairs(m.m.llo, exact_match = TRUE, pars = c("b_sigma_Intercept",
                                            "sd_worker_id__Intercept", 
                                            "sd_worker_id__lo_ground_truth",
                                            "sd_worker_id__sigma_Intercept"))
```


```{r}
pairs(m.m.llo, pars = c("cor_worker_id__"))
```

- Summary

```{r}
# model summary
print(m.m.llo)
```

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select(lo_ground_truth, worker_id, means, sd_diff, condition) %>%
  add_predicted_draws(m.m.llo, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
  ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Running a leave one out posterior predictive check, we can see that overall this model has decent predictive validity.
 
```{r}
# set up data for LOO posterior predictive check
y <- model_df$lo_p_sup
yrep <- posterior_predict(m.m.llo)

# run LOO to get weights
loo <- loo(m.m.llo, save_psis = TRUE, cores = 2)
psis <- loo$psis_object
lw <- weights(psis)
```

```{r}
ppc_loo_pit_qq(y, yrep, lw = lw)
```

Let's take a look at predictions per worker and visualization condition to get a more granular sense of our model fit.

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means, sd_diff, condition) %>%
  add_predicted_draws(m.m.llo, n = 500) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_p_sup, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(model_df$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df$lo_p_sup, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

What does this look like in probability units?

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means, sd_diff, condition) %>%
  add_predicted_draws(m.m.llo, n = 500) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

To examine more closely whether our model has predictive validity at the level of each worker, we'll look at QQ plots for residuals at the worker level.

```{r}
model_check_df %>%
  add_predicted_draws(m.m.llo, n = 500) %>%
  group_by(lo_ground_truth, worker_id) %>%
  summarise(
    p_residual = mean(.prediction < lo_p_sup), # what proportion of predicted judgments are less than the observed response?
    z_residual = qnorm(p_residual)             # what are the z-scores of these cumulative probabilities?
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline() +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

These still look pretty terrible.

What does the posterior for the slope of the LLO model look like when means are present vs absent at different levels of uncertainty, ignoring other manipulations?

```{r}
model_df %>%
  group_by(means, sd_diff, condition) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.m.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, .draw) %>%               # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out visualization condition by taking a weighted average
  ggplot(aes(x = slope, group = means, color = means, fill = means)) +
  geom_density(alpha = 0.35) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes for mean present/absent") +
  theme(panel.grid = element_blank()) +
  facet_grid(. ~ sd_diff)
```

This effect suggests that adding means has a debiasing effect on average when visualized uncertainty is high (marginalizing across visualization conditions). Again, is about what we expected to see. However, we expected the mean to have a biasing effect when uncertainty is low.

Let's look at this difference in a forest plot style display.

```{r}
model_df %>%
  group_by(means, sd_diff, condition) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%                    # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.m.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%            # calculate the difference between fits at 1 and 0 (i.e., slope)
  compare_levels(.value, by = means) %>%                      # look at differences in slopes between means present vs absent
  rename(slope_diff = .value) %>%
  group_by(sd_diff, .draw) %>%                                # group by predictors to keep
  summarise(slope_diff = weighted.mean(slope_diff)) %>%       # marginalize out means present/absent by taking a weighted average
  ggplot(aes(x = slope_diff, y = sd_diff)) +
  stat_halfeyeh() +
  scale_x_continuous(expression(slope_diff), expand = c(0, 0)) +
  labs(subtitle = "Posterior differences in slopes for means present vs absent") +
  theme_bw()
```

What does the posterior for the slope in each visualization condition look like, marginalizing across other factors?

```{r}
model_df %>%
  group_by(means, sd_diff, condition) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.m.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(condition, .draw) %>%                    # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out means present/absent by taking a weighted average
  ggplot(aes(x = slope, group = condition, color = condition, fill = condition)) +
  geom_density(alpha = 0.35) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

Recall that a slope of 1 on the logit scale reflects no bias. This suggests that users are biased toward responses of 50% on the probability scale in all conditions but to different degrees. Quantile dotplots seem to have a substantial debiasing effect on effect size judgments when we marginalize across other manipulations.

What if we break these marginal effects down into simple effects for the interaction of the presence/absence of the mean, level of visualized uncertainty, and visualization condition?

```{r}
model_df %>%
  group_by(means, sd_diff, condition) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.m.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  ggplot(aes(x = slope, group = means, color = means, fill = means)) +
  geom_density(alpha = 0.35) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes for means * sd * visualization condition") +
  theme(panel.grid = element_blank()) +
  facet_grid(condition ~ sd_diff)
```

Again, this is what we expected to see. However, it is not completely clear form this chart if the simple effect of extrinsic means is reliable in some conditions.

Let's look at the differences in a forest plot style display which should make the reliability of these differences a little easier to estimate visually.

```{r}
model_df %>%
  group_by(means, sd_diff, condition) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.m.llo, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  compare_levels(.value, by = means) %>%                 # look at differences in slopes between means present vs absent
  rename(slope_diff = .value) %>%
  unite(cond, condition, sd_diff, sep = "_", remove = FALSE) %>%
  ggplot(aes(x = slope_diff, y = cond)) +
  stat_halfeyeh() +
  scale_x_continuous(expression(slope_diff), expand = c(0, 0)) +
  labs(subtitle = "Posterior differences in slopes for means present vs absent") +
  theme_bw()
```

What is the predicted pattern for responses for the average worker in each cell of this interaction?

```{r}
model_df %>%
  group_by(lo_ground_truth, means, sd_diff, condition) %>%
  add_predicted_draws(m.m.llo, re_formula = NA, n = 500) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(.prediction), color = means, fill = means)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95), alpha = .25) +
  coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) + 
  facet_grid(condition ~ sd_diff)
```

In these plots of the overall response function, we can see that the difference in performance induced by the mean is small relative to the difference between visualization conditions. We can also see that people are by far the least likely to underestimate effect size with quantile dotplots.

Next, we'll try to get more precise estimates by expanding our random effects to include all of the within-subjects manipulations in our study design.


##Building Up Random Effects for Within-Subjects Manipulations

In the minimal model to answer our research questions above, estimates for the effect of means are noisier than we would like, and predictive validity within subjects is not great. We'll try to better account for heterogeneity across subjects by adding more random effects to our model for each within subjects manipulation. 

Following a principle of model expansion, we will make this changes cumulatively. We include a series of model specifications that capture plausible structure in the data and fit without any sampling issues.

###Random Effects for the Interaction of Means and Uncertainty Shown

This first model adds random effects for the within-subjects manipulations in our previous model. We prioritize the interaction between showing means and the level of uncertainty in the distributions since we had a hypothesis about this. We omit the interaction between these terms and the ground truth in the random effects specification because of fit issues: We are unable to identify the random effect of ground truth, means, and level of uncertainty with only one observation per unique combintation of these variables per worker.  

```{r}
# minimal LLO model with random effects for means and sd_diff
m.m.llo.r_means.sd <- brm(data = model_df, family = "gaussian",
                          formula = bf(lo_p_sup ~  (1 + lo_ground_truth + means*sd_diff|sharecor|worker_id) + lo_ground_truth*means*sd_diff*condition,
                                      sigma ~ (1|sharecor|worker_id)),
                          prior = c(prior(normal(1, 0.5), class = b),
                                    prior(normal(1.3, 1), class = Intercept),
                                    prior(normal(0, 0.15), class = sd, group = worker_id),
                                    # prior(normal(0, 0.3), class = b, dpar = sigma),
                                    prior(normal(0, 0.15), class = sd, dpar = sigma),
                                    prior(lkj(4), class = cor)),
                          iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
                          control = list(adapt_delta = 0.99, max_treedepth = 12),
                          file = "model-fits/llo_mdl-min-r_means_sd")
```

```{r}
summary(m.m.llo.r_means.sd)
```

###Mixed Effects of Ground Truth on Sigma

In this model, we add fixed and random effects of ground truth to our sigma submodel. We add a conservative but informative prior in order to model fixed effects on sigma.

```{r}
# minimal LLO model with random effects for means, sd_diff, as well as ground truth for sigma submodel
m.m.llo.r_means.sd.sigma_gt <- brm(data = model_df, family = "gaussian",
                                   formula = bf(lo_p_sup ~  (1 + lo_ground_truth + means*sd_diff|worker_id) + lo_ground_truth*means*sd_diff*condition,
                                                sigma ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth),
                                   prior = c(prior(normal(1, 0.5), class = b),
                                             prior(normal(1.3, 1), class = Intercept),
                                             prior(normal(0, 0.15), class = sd, group = worker_id),
                                             prior(normal(0, 0.3), class = b, dpar = sigma),
                                             prior(normal(0, 0.15), class = sd, dpar = sigma),
                                             prior(lkj(4), class = cor)),
                                   iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
                                   control = list(adapt_delta = 0.99, max_treedepth = 12),
                                   file = "model-fits/llo_mdl-min-r_means_sd_sigma_gt")
```

```{r}
summary(m.m.llo.r_means.sd.sigma_gt)
```

###Mixed Effects of the Interaction of Means and Uncertainty Shown on Sigma

We tried out different models that add mixed effects on residual variance (sigma) for the interaction of extrinsic means and uncertainty shown. We tried many different variations of models with this set of predictors, but we were unable to achieve a usable fit. Multiple versions of the model ran for days before the chains finished sampling. The model below was the best version we managed to fit, but it still has some divergent samples. All of this indicates that we may be better off modeling this data without using means*sd_diff as a predictor of sigma.

```{r}
# minimal LLO model with random effects for means, sd_diff, as well as ground truth, means, sd_diff for sigma submodel
m.m.llo.r_means.sd.sigma_gt.means.sd <- brm(
  data = model_df, family = "gaussian",
  formula = bf(lo_p_sup ~  (1 + lo_ground_truth + means*sd_diff|worker_id) + lo_ground_truth*means*sd_diff*condition,
  sigma ~ (1 + lo_ground_truth + means*sd_diff|worker_id) + lo_ground_truth*means*sd_diff),
  prior = c(prior(normal(1, 0.5), class = b),
            prior(normal(1.3, 1), class = Intercept),
            prior(normal(0, 0.15), class = sd, group = worker_id),
            prior(normal(0, 0.3), class = b, dpar = sigma),
            prior(normal(0, 0.15), class = sd, dpar = sigma),
            prior(lkj(4), class = cor)),
  iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/llo_mdl-min-r_means_sd_sigma_gt_means_sd")
```

```{r}
summary(m.m.llo.r_means.sd.sigma_gt.means.sd)
```

###Mixed Effects of Trial Order on Mean Response

Building on our model with ground truth as a predictor of sigma, this model adds mixed effects of trial order on mean response. This is effectively modeling a learning effect on the mean response at each level of ground truth.

```{r}
# minimal LLO model with random effects for means, sd_diff, trial as well as ground truth for sigma submodel
m.m.llo.r_means.sd.trial.sigma_gt <- brm(
  data = model_df, family = "gaussian",
  formula = bf(lo_p_sup ~  (1 + lo_ground_truth*trial + means*sd_diff|worker_id) + lo_ground_truth*means*sd_diff*condition + lo_ground_truth*condition*trial,
               sigma ~ (1 + lo_ground_truth|worker_id) + lo_ground_truth),
  prior = c(prior(normal(1, 0.5), class = b),
            prior(normal(1.3, 1), class = Intercept),
            prior(normal(0, 0.15), class = sd, group = worker_id),
            prior(normal(0, 0.3), class = b, dpar = sigma),
            prior(normal(0, 0.15), class = sd, dpar = sigma),
            prior(lkj(4), class = cor)),
  iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/llo_mdl-min-r_means_sd_trial_sigma_gt3")
```

```{r}
summary(m.m.llo.r_means.sd.trial.sigma_gt)
```

###Mixed Effects of Trial Order on Sigma

This model adds fixed and random intercepts of trial order to sigma submodel. This is a a learning effect on residual variance.

```{r}
# minimal LLO model with random effects for means, sd_diff, trial as well as ground truth for sigma submodel
m.m.llo.r_means.sd.trial.sigma_gt.trial <- brm(
  data = model_df, family = "gaussian",
  formula = bf(lo_p_sup ~  (1 + lo_ground_truth*trial + means*sd_diff|worker_id) + lo_ground_truth*means*sd_diff*condition + lo_ground_truth*condition*trial,
               sigma ~ (1 + lo_ground_truth + trial|worker_id) + lo_ground_truth*condition*trial),
  prior = c(prior(normal(1, 0.5), class = b),
            prior(normal(1.3, 1), class = Intercept),
            prior(normal(0, 0.15), class = sd, group = worker_id),
            prior(normal(0, 0.3), class = b, dpar = sigma),
            prior(normal(0, 0.15), class = sd, dpar = sigma),
            prior(lkj(4), class = cor)),
  iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
  control = list(adapt_delta = 0.99, max_treedepth = 12),
  file = "model-fits/llo_mdl-min-r_means_sd_trial_sigma_gt_trial3b")
```

```{r}
summary(m.m.llo.r_means.sd.trial.sigma_gt.trial)
```


###Model Comparison

Each time we add a random effect, the number of parameters multiplies, especially since the random effects in each submodel share a covariance matrix. We want to make sure these parameters are contributing to the predictive validity of the model more than they risk overfitting. We'll evaluate this by using WAIC to compare models. Whichever model has the smallest value of WAIC is the one that has the best predictive validity for the fewest parameters.

```{r}
waic(
  m.m.llo,
  m.m.llo.r_means.sd,
  m.m.llo.r_means.sd.sigma_gt,
  m.m.llo.r_means.sd.trial.sigma_gt,
  m.m.llo.r_means.sd.trial.sigma_gt.trial)
```

The most complex model has the lowest WAIC value, so we'll continue expanding on it.

<!-- ###Predictive Checks -->

<!-- Let's check our posterior predictive distribution. -->

<!-- ```{r} -->
<!-- # posterior predictive check -->
<!-- model_df %>% -->
<!--   select(lo_ground_truth, worker_id, means, sd_diff, condition, trial) %>% -->
<!--   add_predicted_draws(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial, prediction = "lo_p_sup", seed = 1234) %>% -->
<!--   mutate( -->
<!--     # transform to probability units -->
<!--     post_p_sup = plogis(lo_p_sup) -->
<!--   ) %>% -->
<!--   ggplot(aes(x = post_p_sup)) + -->
<!--   geom_density(fill = "black", size = 0) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   labs(subtitle = "Posterior predictive distribution for probability of superiority") + -->
<!--   theme(panel.grid = element_blank()) -->
<!-- ``` -->

<!-- How do these predictions compare to the observed data? -->

<!-- ```{r} -->
<!-- # data density -->
<!-- model_df %>% -->
<!--   ggplot(aes(x = p_superiority)) + -->
<!--   geom_density(fill = "black", size = 0) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   labs(subtitle = "Data distribution for probability of superiority") + -->
<!--   theme(panel.grid = element_blank()) -->
<!-- ``` -->

<!-- Running a leave one out posterior predictive check, we can see that overall this model has decent predictive validity. -->

<!-- ```{r} -->
<!-- # set up data for LOO posterior predictive check -->
<!-- y <- model_df$lo_p_sup -->
<!-- yrep <- posterior_predict(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial) -->

<!-- # run LOO to get weights -->
<!-- loo <- loo(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial, save_psis = TRUE, cores = 2) -->
<!-- psis <- loo$psis_object -->
<!-- lw <- weights(psis) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ppc_loo_pit_qq(y, yrep, lw = lw) -->
<!-- ``` -->

<!-- Let's take a look at predictions per worker and visualization condition to get a more granular sense of our model fit. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(lo_ground_truth, worker_id, means, sd_diff, condition) %>% -->
<!--   add_predicted_draws(m.wrkr.means.sd.vis.llo_p_sup) %>% -->
<!--   ggplot(aes(x = lo_ground_truth, y = lo_p_sup, color = condition, fill = condition)) + -->
<!--   geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth -->
<!--   stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = .25) + -->
<!--   geom_point(data = model_df) + -->
<!--   scale_fill_brewer(type = "qual", palette = 1) + -->
<!--   scale_color_brewer(type = "qual", palette = 1) +  -->
<!--   coord_cartesian(xlim = quantile(model_df$lo_ground_truth, c(0, 1)), -->
<!--                   ylim = quantile(model_df$lo_p_sup, c(0, 1))) + -->
<!--   theme_bw() + -->
<!--   theme(panel.grid = element_blank()) +  -->
<!--   facet_wrap(~ worker_id) -->
<!-- ``` -->

<!-- What does this look like in probability units? -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(lo_ground_truth, worker_id, means, sd_diff, condition) %>% -->
<!--   add_predicted_draws(m.wrkr.means.sd.vis.llo_p_sup) %>% -->
<!--   ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = condition, fill = condition)) + -->
<!--   geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth -->
<!--   stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = .25) + -->
<!--   geom_point(data = model_df) + -->
<!--   scale_fill_brewer(type = "qual", palette = 1) + -->
<!--   scale_color_brewer(type = "qual", palette = 1) +  -->
<!--   coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)), -->
<!--                   ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) + -->
<!--   theme_bw() + -->
<!--   theme(panel.grid = element_blank()) +  -->
<!--   facet_wrap(~ worker_id) -->
<!-- ``` -->

<!-- To examine more closely whether our model has predictive validity at the level of each worker, we'll look at QQ plots for residuals at the worker level. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   add_predicted_draws(m.wrkr.means.sd.vis.llo_p_sup) %>% -->
<!--   group_by(lo_ground_truth, worker_id) %>% -->
<!--   summarise( -->
<!--     p_residual = mean(.prediction < lo_p_sup), # what proportion of predicted judgments are less than the observed response? -->
<!--     z_residual = qnorm(p_residual)             # what are the z-scores of these cumulative probabilities? -->
<!--   ) %>% -->
<!--   ggplot(aes(sample = z_residual)) + -->
<!--   geom_qq() + -->
<!--   geom_abline() + -->
<!--   theme_bw() + -->
<!--   theme(panel.grid = element_blank()) +  -->
<!--   facet_wrap(~ worker_id) -->
<!-- ``` -->

<!-- These still look pretty terrible. -->

<!-- What does the posterior for the slope of the LLO model look like when means are present vs absent at different levels of uncertainty, ignoring other manipulations? -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(means, sd_diff, condition, trial) %>% -->
<!--   data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1 -->
<!--   add_fitted_draws(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial, re_formula = NA) %>% -->
<!--   compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope) -->
<!--   rename(slope = .value) %>% -->
<!--   group_by(means, sd_diff, .draw) %>%               # group by predictors to keep -->
<!--   summarise(slope = weighted.mean(slope)) %>%       # marginalize out visualization condition by taking a weighted average -->
<!--   ggplot(aes(x = slope, group = means, color = means, fill = means)) + -->
<!--   geom_density(alpha = 0.35) + -->
<!--   scale_x_continuous(expression(slope), expand = c(0, 0)) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   labs(subtitle = "Posterior for slopes for mean present/absent") + -->
<!--   theme(panel.grid = element_blank()) + -->
<!--   facet_grid(. ~ sd_diff) -->
<!-- ``` -->

<!-- This effect suggests that adding means has a debiasing effect on average when visualized uncertainty is high and a biasing effect when uncertainty is low (marginalizing across visualization conditions). Our estimate is more precise with the more complex model. -->

<!-- Let's look at this difference in a forest plot style display. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(means, sd_diff, condition) %>% -->
<!--   data_grid(lo_ground_truth = c(0, 1)) %>%                    # get fitted draws (in log odds units) only for ground truth of 0 and 1 -->
<!--   add_fitted_draws(m.wrkr.means.sd.vis.llo_p_sup, re_formula = NA) %>% -->
<!--   compare_levels(.value, by = lo_ground_truth) %>%            # calculate the difference between fits at 1 and 0 (i.e., slope) -->
<!--   compare_levels(.value, by = means) %>%                      # look at differences in slopes between means present vs absent -->
<!--   rename(slope_diff = .value) %>% -->
<!--   group_by(sd_diff, .draw) %>%                                # group by predictors to keep -->
<!--   summarise(slope_diff = weighted.mean(slope_diff)) %>%       # marginalize out means present/absent by taking a weighted average -->
<!--   ggplot(aes(x = slope_diff, y = sd_diff)) + -->
<!--   stat_halfeyeh() + -->
<!--   scale_x_continuous(expression(slope_diff), expand = c(0, 0)) + -->
<!--   labs(subtitle = "Posterior differences in slopes for means present vs absent") + -->
<!--   theme_bw() -->
<!-- ``` -->

<!-- What does the posterior for the slope in each visualization condition look like, marginalizing across other factors? -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(means, sd_diff, condition) %>% -->
<!--   data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1 -->
<!--   add_fitted_draws(m.wrkr.means.sd.vis.llo_p_sup, re_formula = NA) %>% -->
<!--   compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope) -->
<!--   rename(slope = .value) %>% -->
<!--   group_by(condition, .draw) %>%                    # group by predictors to keep -->
<!--   summarise(slope = weighted.mean(slope)) %>%       # marginalize out means present/absent by taking a weighted average -->
<!--   ggplot(aes(x = slope, group = condition, color = condition, fill = condition)) + -->
<!--   geom_density(alpha = 0.35) + -->
<!--   scale_fill_brewer(type = "qual", palette = 1) + -->
<!--   scale_color_brewer(type = "qual", palette = 1) + -->
<!--   scale_x_continuous(expression(slope), expand = c(0, 0)) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   labs(subtitle = "Posterior for slopes by visualization condition") + -->
<!--   theme(panel.grid = element_blank()) -->
<!-- ``` -->

<!-- Recall that a slope of 1 on the logit scale reflects no bias. This suggests that users are biased toward responses of 50% on the probability scale in both conditions, but especially with intervals. HOPs seem to have a substantial debiasing effect on effect size judgments when we marginalize across other manipulations. -->

<!-- What if we break these marginal effects down into simple effects for the interaction of the presence/absence of the mean, level of visualized uncertainty, and visualization condition? -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(means, sd_diff, condition, trial) %>% -->
<!--   data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1 -->
<!--   add_fitted_draws(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial, re_formula = NA) %>% -->
<!--   compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope) -->
<!--   rename(slope = .value) %>% -->
<!--   group_by(means, sd_diff, condition, .draw) %>%    # group by predictors to keep -->
<!--   summarise(slope = weighted.mean(slope)) %>%       # marginalize out means present/absent by taking a weighted average -->
<!--   ggplot(aes(x = slope, group = means, color = means, fill = means)) + -->
<!--   geom_density(alpha = 0.35) + -->
<!--   scale_x_continuous(expression(slope), expand = c(0, 0)) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   labs(subtitle = "Posterior for slopes for means * sd * visualization condition") + -->
<!--   theme(panel.grid = element_blank()) + -->
<!--   facet_grid(condition ~ sd_diff) -->
<!-- ``` -->

<!-- Again, this is what we expected to see. However, it is not clear form this chart if the effect is reliable. -->

<!-- Let's look at the differences in a forest plot style display. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(means, sd_diff, condition, trial) %>% -->
<!--   data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1 -->
<!--   add_fitted_draws(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial, re_formula = NA) %>% -->
<!--   compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope) -->
<!--   rename(slope = .value) %>% -->
<!--   group_by(means, sd_diff, condition, .draw) %>%     # group by predictors to keep -->
<!--   summarise(slope = weighted.mean(slope)) %>%       # marginalize by taking a weighted average -->
<!--   compare_levels(slope, by = means) %>%             # look at differences in slopes between means present vs absent -->
<!--   rename(slope_diff = slope) %>% -->
<!--   unite(cond, condition, sd_diff, sep = "_", remove = FALSE) %>% -->
<!--   ggplot(aes(x = slope_diff, y = cond)) + -->
<!--   stat_halfeyeh() + -->
<!--   scale_x_continuous(expression(slope_diff), expand = c(0, 0)) + -->
<!--   labs(subtitle = "Posterior differences in slopes for means present vs absent") + -->
<!--   theme_bw() -->
<!-- ``` -->

<!-- What is the predicted pattern for responses for the average worker in each cell of this interaction? -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(lo_ground_truth, means, sd_diff, condition, trial) %>% -->
<!--   add_predicted_draws(m.wrkr.means.sd.vis.llo_p_sup.r.means.sd.trial.sigma.gt.means.sd.trial, re_formula = NA) %>% -->
<!--   ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = means, fill = means)) + -->
<!--   geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth -->
<!--   stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95), alpha = .25) + -->
<!--   coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)), -->
<!--                   ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) + -->
<!--   theme_bw() + -->
<!--   theme(panel.grid.minor = element_blank()) +  -->
<!--   facet_wrap(condition ~ sd_diff) -->
<!-- ``` -->


##Add Predictors for Block Order

Let's add block order to our previous model, just to check if the effect of the mean on judgments depends on block order. We'll model this as a fixed effects interaction between block order and the presence absence of means. This will be the maximal model under our stategy of model expansion.

We use the same priors as we did for the previous model. Now, let's fit the model to our data.

```{r}
# hierarchical LLO model
m.max <- brm(data = model_df, family = "gaussian",
             formula = bf(lo_p_sup ~  (1 + lo_ground_truth*trial + means*sd_diff|worker_id) + lo_ground_truth*means*sd_diff*condition*start_means + lo_ground_truth*condition*trial,
                          sigma ~ (1 + lo_ground_truth + trial|worker_id) + lo_ground_truth*condition*trial + means*start_means),
             prior = c(prior(normal(1, 0.5), class = b),
                       prior(normal(1.3, 1), class = Intercept),
                       prior(normal(0, 0.15), class = sd, group = worker_id),
                       prior(normal(0, 0.3), class = b, dpar = sigma),
                       prior(normal(0, 0.15), class = sd, dpar = sigma),
                       prior(lkj(4), class = cor)),
             iter = 12000, warmup = 2000, chains = 2, cores = 2, thin = 2,
             control = list(adapt_delta = 0.99, max_treedepth = 12),
             file = "model-fits/llo_mdl-min-r_means_sd_trial_block_sigma_gt_trial_means_block-build_version")
```

```{r}
summary(m.max)
```

###Model Comparison

Let's see how this maximal model compares with our previous model.

```{r}
waic(m.m.llo.r_means.sd.trial.sigma_gt.trial, m.max)
```

It looks like adding predictors for block order improves fit somewhat, so we'll run with the maximal version of the model that we managed to fit.

###Predictive Checks

Let's check our posterior predictive distribution.

```{r}
# posterior predictive check
model_df %>%
  select(lo_ground_truth, worker_id, means, sd_diff, condition, trial, start_means) %>%
  add_predicted_draws(m.max, prediction = "lo_p_sup", seed = 1234, n = 500) %>%
  mutate(
    # transform to probability units
    post_p_sup = plogis(lo_p_sup)
  ) %>%
  ggplot(aes(x = post_p_sup)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior predictive distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

How do these predictions compare to the observed data?

```{r}
# data density
model_df %>%
  ggplot(aes(x = p_superiority)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Data distribution for probability of superiority") +
  theme(panel.grid = element_blank())
```

Let's take a look at predictions per worker and visualization condition to get a more granular sense of our model fit.

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means, sd_diff, condition, trial, start_means) %>%
  add_predicted_draws(m.max, n = 500) %>%
  ggplot(aes(x = lo_ground_truth, y = lo_p_sup, color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(model_df$lo_ground_truth, c(0, 1)),
                  ylim = quantile(model_df$lo_p_sup, c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

What does this look like in probability units?

```{r}
model_check_df %>%
  group_by(lo_ground_truth, worker_id, means, sd_diff, condition, trial, start_means) %>%
  add_predicted_draws(m.max, n = 500) %>%
  ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), color = condition, fill = condition)) +
  geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth
  stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95, .80, .50), alpha = .25) +
  geom_point(data = model_check_df) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) + 
  coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)),
                  ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) +
  theme_bw() +
  theme(panel.grid = element_blank()) + 
  facet_wrap(~ worker_id)
```

####Order Effects

What does the posterior for the slope look like when means are present vs absent? We'll split this based on uncertainty shown and block order (marginalizing across visualization conditions) to see if there is a difference in the effect of extrinsic means per block.

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, start_means, .draw) %>%  # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out visualization condition by taking a weighted average
  ggplot(aes(x = slope, group = means, color = means, fill = means)) +
  geom_density(alpha = 0.35) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes for mean present/absent") +
  theme(panel.grid = element_blank()) +
  facet_grid(start_means ~ sd_diff)
```

This effect suggests that adding means is most harmful at low uncertainty when users start with them, and adding means is helpful at high uncertainty in the second block of trials. This is a strange order effect, and it may be burying the signal for the 

What does the posterior for the slope in each visualization condition look like, marginalizing across other predictors? Again, we'll facet by block order to see if this has any impact on our results.

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(condition, start_means, .draw) %>%       # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out means present/absent by taking a weighted average
  ggplot(aes(x = slope, group = condition, color = condition, fill = condition)) +
  geom_density(alpha = 0.35) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank()) +
  facet_grid(start_means ~ .)
```

It looks like LLO slopes are smaller (more biased) when users start the task with extrinsic means, except for with quantile dotplots. 

What if we break these marginal effects down into simple effects for the interaction of the presence/absence of the mean, uncertainty shown, block order, and visualization condition?

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%                      # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%              # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, condition, start_means, .draw) %>%   # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%                   # marginalize out means present/absent by taking a weighted average
  ggplot(aes(x = slope, y = condition, group = means, fill = means)) +
  stat_slabh(alpha = 0.35) + 
  labs(subtitle = "Posterior for slopes for means * sd * block order * visualization condition") +
  theme_minimal() +
  facet_grid(start_means ~ sd_diff)
```

It looks like when participants start the task with extrinsic means, their LLO slopes become less biased when those means are removed, especially when uncertainty is low. Whereas when participants start the task without means, LLO slopes become less biased when means are added only for intervals and densities at high levels of uncertainty. For HOPs on the other hand, adding extrinsic means in the second block makes slopes more biased (despite the fact the users have more practice with HOPs by the second block).

####Main Findings Adjusting for Order Effects

What is the effect of extrinsic means at high and low undertainty in our four visualization condition after adjusting for order effects?

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%                      # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%              # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, condition, .draw) %>%   # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%      # marginalize out other predictors by taking a weighted average
  ggplot(aes(x = slope, y = condition, group = means, fill = means)) +
  stat_slabh(alpha = 0.35) +
  labs(
    title = "Posterior Slopes in Linear Log Odds Model",
    x = "Slope",
    y = "Visualization",
    fill = "Means Present"
  ) +
  theme_minimal() +
  # theme(panel.grid.minor = element_blank()) +
  facet_grid(. ~ sd_diff)
```

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%                      # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%              # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, condition, .draw) %>%   # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%      # marginalize out other predictors by taking a weighted average
  compare_levels(slope, by = means) %>%            # contrast mean present - absent
  ggplot(aes(x = slope, y = condition)) +
  stat_slabh(alpha = 0.35) +
  labs(
    title = "Effect of Means on LLO Slopes",
    x = "Slope Difference (Means present - absent)",
    y = "Visualization"
  ) +
  theme_minimal() +
  # theme(panel.grid.minor = element_blank()) +
  facet_grid(. ~ sd_diff)
```

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%                      # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%              # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(means, sd_diff, condition, .draw) %>%   # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%      # marginalize out other predictors by taking a weighted average
  compare_levels(slope, by = means) %>%            # contrast mean present - absent
  compare_levels(slope, by = sd_diff) %>%            # contrast sd_diff high - low (I think)
  ggplot(aes(x = slope, y = condition)) +
  stat_slabh(alpha = 0.35) +
  labs(
    title = "Posterior Slopes in Linear Log Odds Model",
    x = "Slope Difference (Effect of means at high - low uncertainty)",
    y = "Visualization"
  ) +
  theme_minimal()
  # theme(panel.grid.minor = element_blank())
```

It looks like extrinsic means lead to greater underestimation of probability of superiority (lower LLO slopes) when uncertainty is low, regardless of visualization condition. This is the effect we expected to see but which eluded us until we controlled for order effects. Surprisingly, the impact of extrinsic means does not seem to depend on the intinsic salience of the mean in the uncertainty visualization conditions. At high levels of uncertainty, extrinsic means improve slopes for intervals and densities but still reduce slopes for HOPs. *These results suggest that adding extrinsic means is not a good design choice for HOPs or when the distributions visualized on a common axis differ in their variance*.

What about the slopes in each visualization condition after adjusting for order effects?

```{r}
model_df %>%
  group_by(means, sd_diff, condition, trial, start_means) %>%
  data_grid(lo_ground_truth = c(0, 1)) %>%          # get fitted draws (in log odds units) only for ground truth of 0 and 1
  add_fitted_draws(m.max, re_formula = NA) %>%
  compare_levels(.value, by = lo_ground_truth) %>%  # calculate the difference between fits at 1 and 0 (i.e., slope)
  rename(slope = .value) %>%
  group_by(condition, .draw) %>%       # group by predictors to keep
  summarise(slope = weighted.mean(slope)) %>%       # marginalize out means present/absent by taking a weighted average
  ggplot(aes(x = slope, group = condition, color = condition, fill = condition)) +
  geom_density(alpha = 0.35) +
  scale_fill_brewer(type = "qual", palette = 2) +
  scale_color_brewer(type = "qual", palette = 2) +
  scale_x_continuous(expression(slope), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Posterior for slopes by visualization condition") +
  theme(panel.grid = element_blank())
```

<!-- ####Showing Model Predictions Instead of Slopes -->

<!-- Another way to understand predictions from this model is to plot posterior predictions for each condition, instead of just slopes. Let's take a look at predictions per visualization condition in probability units for the average worker. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(lo_ground_truth, means, sd_diff, condition, trial, start_means) %>% -->
<!--   add_predicted_draws(m.max, re_formula = NA, n = 500) %>% -->
<!--   ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup))) + -->
<!--   geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth -->
<!--   stat_lineribbon(aes(y = plogis(.prediction)), .width = c(.95), alpha = .25, fill = "black", show.legend = FALSE) + -->
<!--   coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)), -->
<!--                   ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) + -->
<!--   theme_minimal() + -->
<!--   # theme(panel.grid.minor = element_blank()) +  -->
<!--   facet_grid(condition ~ sd_diff) -->
<!-- ``` -->

<!-- We can see that intervals lead to less biased perceptions, especially when means are added in the second block. -->

<!-- Let's also look at a spaghetti plot of average predictions per worker. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   add_predicted_draws(m.max, n = 500) %>% -->
<!--   group_by(lo_ground_truth, worker_id, means, sd_diff, condition, start_means) %>% # marginalize over trial order -->
<!--   summarize(avg_pred = weighted.mean(.prediction)) %>% -->
<!--   unite(vis_cond, condition, sd_diff) %>% -->
<!--   unite(means_present, means, start_means) %>% -->
<!--   ggplot(aes(x = plogis(lo_ground_truth), y = plogis(lo_p_sup), group = worker_id)) + -->
<!--   geom_abline(intercept = 0, slope = 1, size = 1, alpha = .3, color = "red", linetype = "dashed") + # ground truth -->
<!--   geom_line(aes(y = plogis(avg_pred)), alpha = .65) + -->
<!--   geom_point(data = model_df, alpha = 0.25) + -->
<!--   coord_cartesian(xlim = quantile(plogis(model_df$lo_ground_truth), c(0, 1)), -->
<!--                   ylim = quantile(plogis(model_df$lo_p_sup), c(0, 1))) + -->
<!--   theme_bw() + -->
<!--   theme(panel.grid.minor = element_blank()) +  -->
<!--   facet_grid(means_present ~ vis_cond) -->
<!-- ``` -->

<!-- To get a better sense of practical guidance for visualization practioners, let's break down these effects into low, mid, and high ground truth effect size and look at the impact of each uncertainty encoding on error in magnitude estimation when uncertainty is high vs low. -->

<!-- ```{r} -->
<!-- model_df %>% -->
<!--   group_by(lo_ground_truth, means, sd_diff, condition) %>% -->
<!--   add_predicted_draws(m.m.llo, re_formula = NA, n = 500) %>% -->
<!--   mutate( -->
<!--     # bin ground truth -->
<!--     effect_size = if_else(ground_truth > 0.5 & ground_truth <= 0.65, -->
<!--                           "low", -->
<!--                           if_else(ground_truth > 0.65 & ground_truth <= 0.8, -->
<!--                                   "mid", -->
<!--                                   if_else(ground_truth > 0.8 & ground_truth <= 1, -->
<!--                                           "high", -->
<!--                                           "exclude"))), -->
<!--     effect_size = factor(effect_size, levels = c("low", "mid", "high")), -->
<!--     # calculate estimation error -->
<!--     est_error = plogis(.prediction) - ground_truth -->
<!--   ) %>% -->
<!--   filter(effect_size != "exclude") %>% -->
<!--   ggplot(aes(x = est_error, y = condition, fill = condition)) + -->
<!--   stat_slabh(alpha = 0.35) +  -->
<!--   scale_fill_brewer(type = "qual", palette = 2) + -->
<!--   theme_minimal() + -->
<!--   # theme(panel.grid.minor = element_blank()) + -->
<!--   facet_grid(effect_size ~ sd_diff) -->
<!-- ``` -->